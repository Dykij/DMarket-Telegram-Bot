"""
Llama 3.1 8B Integration Module for DMarket Telegram Bot.

–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–æ–¥—É–ª—å –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ Llama 3.1 8B (Q4 –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è) —Å –±–æ—Ç–æ–º.
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è Ryzen 7 5700X, 32GB RAM, RX 6600 (8GB VRAM).

Features:
- Market analysis (–∞–Ω–∞–ª–∏–∑ —Ä—ã–Ω–∫–∞)
- Price prediction (–ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ —Ü–µ–Ω)
- Arbitrage recommendations (—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∞—Ä–±–∏—Ç—Ä–∞–∂—É)
- Trading advice (—Ç–æ—Ä–≥–æ–≤—ã–µ —Å–æ–≤–µ—Ç—ã)
- Natural language queries (–∑–∞–ø—Ä–æ—Å—ã –Ω–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —è–∑—ã–∫–µ)
"""

import asyncio
import json
from dataclasses import dataclass
from datetime import datetime
from enum import Enum
from typing import Any

import structlog

try:
    import httpx
    HTTPX_AVAILABLE = True
except ImportError:
    HTTPX_AVAILABLE = False


logger = structlog.get_logger(__name__)


class LlamaTaskType(str, Enum):
    """–¢–∏–ø—ã –∑–∞–¥–∞—á –¥–ª—è Llama."""
    
    MARKET_ANALYSIS = "market_analysis"
    PRICE_PREDICTION = "price_prediction"
    ARBITRAGE_RECOMMENDATION = "arbitrage_recommendation"
    TRADING_ADVICE = "trading_advice"
    GENERAL_CHAT = "general_chat"
    ITEM_EVALUATION = "item_evaluation"
    RISK_ASSESSMENT = "risk_assessment"


@dataclass
class LlamaConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Llama –º–æ–¥–µ–ª–∏."""
    
    model_name: str = "llama3.1:8b"
    ollama_url: str = "http://localhost:11434"
    temperature: float = 0.7
    top_p: float = 0.9
    max_tokens: int = 1024
    timeout: float = 120.0
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è Q4 –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏
    quantization: str = "Q4_K_M"
    context_length: int = 8192
    
    # Hardware recommendations
    min_vram_gb: int = 6
    recommended_vram_gb: int = 8
    cpu_threads: int = 8


@dataclass
class LlamaResponse:
    """–û—Ç–≤–µ—Ç –æ—Ç Llama."""
    
    success: bool
    response: str
    task_type: LlamaTaskType
    tokens_used: int = 0
    processing_time_ms: float = 0
    error: str | None = None
    metadata: dict[str, Any] | None = None


# –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á
TASK_PROMPTS = {
    LlamaTaskType.MARKET_ANALYSIS: """–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É —Ä—ã–Ω–∫–∞ –∏–≥—Ä–æ–≤—ã—Ö —Å–∫–∏–Ω–æ–≤ DMarket.
–ê–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ä—ã–Ω–æ–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:
- –û–ø—Ä–µ–¥–µ–ª—è–π —Ç—Ä–µ–Ω–¥—ã (—Ä–æ—Å—Ç/–ø–∞–¥–µ–Ω–∏–µ/—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å)
- –í—ã—è–≤–ª—è–π –∞–Ω–æ–º–∞–ª–∏–∏ —Ü–µ–Ω
- –û—Ü–µ–Ω–∏–≤–∞–π –ª–∏–∫–≤–∏–¥–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–º–µ—Ç–æ–≤
- –î–∞–≤–∞–π –ø—Ä–æ–≥–Ω–æ–∑—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞:
üìä –¢–†–ï–ù–î: [–≤–æ—Å—Ö–æ–¥—è—â–∏–π/–Ω–∏—Å—Ö–æ–¥—è—â–∏–π/–±–æ–∫–æ–≤–æ–π]
üìà –°–ò–õ–ê –¢–†–ï–ù–î–ê: [—Å–∏–ª—å–Ω—ã–π/—É–º–µ—Ä–µ–Ω–Ω—ã–π/—Å–ª–∞–±—ã–π]
üí∞ –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–Ø: [–ø–æ–∫—É–ø–∞—Ç—å/–ø—Ä–æ–¥–∞–≤–∞—Ç—å/–¥–µ—Ä–∂–∞—Ç—å]
‚ö†Ô∏è –†–ò–°–ö: [–Ω–∏–∑–∫–∏–π/—Å—Ä–µ–¥–Ω–∏–π/–≤—ã—Å–æ–∫–∏–π]
üìù –ê–ù–ê–õ–ò–ó: [–ø–æ–¥—Ä–æ–±–Ω—ã–π –∞–Ω–∞–ª–∏–∑]""",

    LlamaTaskType.PRICE_PREDICTION: """–¢—ã - AI –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è —Ü–µ–Ω –Ω–∞ –∏–≥—Ä–æ–≤—ã–µ –ø—Ä–µ–¥–º–µ—Ç—ã.
–ù–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏ —Ç–µ–∫—É—â–∏—Ö —Ç—Ä–µ–Ω–¥–æ–≤:
- –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–π –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ü–µ–Ω—ã –Ω–∞ 24—á/7–¥/30–¥
- –û–ø—Ä–µ–¥–µ–ª—è–π —É—Ä–æ–≤–Ω–∏ –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –∏ —Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏—è
- –û—Ü–µ–Ω–∏–≤–∞–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø—Ä–æ–≥–Ω–æ–∑–∞

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞:
üéØ –ü–†–û–ì–ù–û–ó 24—á: [—Ü–µ–Ω–∞] ([+/-]X%)
üéØ –ü–†–û–ì–ù–û–ó 7–¥: [—Ü–µ–Ω–∞] ([+/-]X%)
üéØ –ü–†–û–ì–ù–û–ó 30–¥: [—Ü–µ–Ω–∞] ([+/-]X%)
üìä –£–†–û–í–ï–ù–¨ –ü–û–î–î–ï–†–ñ–ö–ò: [—Ü–µ–Ω–∞]
üìä –£–†–û–í–ï–ù–¨ –°–û–ü–†–û–¢–ò–í–õ–ï–ù–ò–Ø: [—Ü–µ–Ω–∞]
üîÆ –£–í–ï–†–ï–ù–ù–û–°–¢–¨: [–Ω–∏–∑–∫–∞—è/—Å—Ä–µ–¥–Ω—è—è/–≤—ã—Å–æ–∫–∞—è]
üìù –û–ë–û–°–ù–û–í–ê–ù–ò–ï: [–∞–Ω–∞–ª–∏–∑]""",

    LlamaTaskType.ARBITRAGE_RECOMMENDATION: """–¢—ã - —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç –ø–æ –∞—Ä–±–∏—Ç—Ä–∞–∂—É –º–µ–∂–¥—É –ø–ª–æ—â–∞–¥–∫–∞–º–∏.
–ü–ª–æ—â–∞–¥–∫–∏ –∏ –∫–æ–º–∏—Å—Å–∏–∏:
- DMarket: 7%
- Waxpeer: 6%
- Steam Market: 15%

–ê–Ω–∞–ª–∏–∑–∏—Ä—É–π –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∞—Ä–±–∏—Ç—Ä–∞–∂–∞:
- –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–π —á–∏—Å—Ç—É—é –ø—Ä–∏–±—ã–ª—å —Å —É—á–µ—Ç–æ–º –∫–æ–º–∏—Å—Å–∏–π
- –û—Ü–µ–Ω–∏–≤–∞–π —Ä–∏—Å–∫–∏ (–ª–∏–∫–≤–∏–¥–Ω–æ—Å—Ç—å, –≤—Ä–µ–º—è –ø—Ä–æ–¥–∞–∂–∏)
- –†–∞–Ω–∂–∏—Ä—É–π –ø–æ ROI

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞:
üíé –í–û–ó–ú–û–ñ–ù–û–°–¢–¨: [–æ–ø–∏—Å–∞–Ω–∏–µ]
üí∞ –ß–ò–°–¢–ê–Ø –ü–†–ò–ë–´–õ–¨: $X.XX (Y%)
üìà ROI: Z%
‚è±Ô∏è –í–†–ï–ú–Ø –†–ï–ê–õ–ò–ó–ê–¶–ò–ò: [—á–∞—Å—ã/–¥–Ω–∏]
‚ö†Ô∏è –†–ò–°–ö: [–Ω–∏–∑–∫–∏–π/—Å—Ä–µ–¥–Ω–∏–π/–≤—ã—Å–æ–∫–∏–π]
‚úÖ –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–Ø: [–¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å/–ø–æ–¥–æ–∂–¥–∞—Ç—å/–ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å]""",

    LlamaTaskType.TRADING_ADVICE: """–¢—ã - —Ç–æ—Ä–≥–æ–≤—ã–π —Å–æ–≤–µ—Ç–Ω–∏–∫ –¥–ª—è DMarket –±–æ—Ç–∞.
–î–∞–≤–∞–π —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ:
- –ú–æ–º–µ–Ω—Ç—É –≤—Ö–æ–¥–∞/–≤—ã—Ö–æ–¥–∞ –∏–∑ –ø–æ–∑–∏—Ü–∏–∏
- –†–∞–∑–º–µ—Ä—É –ø–æ–∑–∏—Ü–∏–∏
- –£–ø—Ä–∞–≤–ª–µ–Ω–∏—é —Ä–∏—Å–∫–∞–º–∏
- –î–∏–≤–µ—Ä—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–æ—Ä—Ç—Ñ–µ–ª—è

–£—Ä–æ–≤–Ω–∏ –∞—Ä–±–∏—Ç—Ä–∞–∂–∞:
- boost: $0.50-$3 (–Ω–∞—á–∏–Ω–∞—é—â–∏–µ)
- standard: $3-$10 (—Å—Ç–∞–Ω–¥–∞—Ä—Ç)
- medium: $10-$30 (—Å—Ä–µ–¥–Ω–∏–π)
- advanced: $30-$100 (–ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π)
- pro: $100+ (–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª)

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞:
üéØ –°–û–í–ï–¢: [–∫—Ä–∞—Ç–∫–∏–π —Å–æ–≤–µ—Ç]
üìä –ü–û–ó–ò–¶–ò–Ø: [–æ—Ç–∫—Ä—ã—Ç—å/–∑–∞–∫—Ä—ã—Ç—å/–¥–µ—Ä–∂–∞—Ç—å]
üí∞ –†–ê–ó–ú–ï–†: [% –æ—Ç –±–∞–ª–∞–Ω—Å–∞]
‚ö†Ô∏è –°–¢–û–ü-–õ–û–°–°: [—É—Ä–æ–≤–µ–Ω—å]
üéØ –¢–ï–ô–ö-–ü–†–û–§–ò–¢: [—É—Ä–æ–≤–µ–Ω—å]
üìù –ü–û–Ø–°–ù–ï–ù–ò–ï: [–¥–µ—Ç–∞–ª–∏]""",

    LlamaTaskType.ITEM_EVALUATION: """–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –æ—Ü–µ–Ω–∫–µ –∏–≥—Ä–æ–≤—ã—Ö –ø—Ä–µ–¥–º–µ—Ç–æ–≤.
–û—Ü–µ–Ω–∏–≤–∞–π –ø—Ä–µ–¥–º–µ—Ç—ã –ø–æ –∫—Ä–∏—Ç–µ—Ä–∏—è–º:
- –†–µ–¥–∫–æ—Å—Ç—å –∏ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å
- –ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∞—è –¥–∏–Ω–∞–º–∏–∫–∞ —Ü–µ–Ω
- –õ–∏–∫–≤–∏–¥–Ω–æ—Å—Ç—å –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –ø–ª–æ—â–∞–¥–∫–∞—Ö
- –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª —Ä–æ—Å—Ç–∞/–ø–∞–¥–µ–Ω–∏—è

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞:
üè∑Ô∏è –ü–†–ï–î–ú–ï–¢: [–Ω–∞–∑–≤–∞–Ω–∏–µ]
üí∞ –°–ü–†–ê–í–ï–î–õ–ò–í–ê–Ø –¶–ï–ù–ê: $X.XX
üìä –õ–ò–ö–í–ò–î–ù–û–°–¢–¨: [–≤—ã—Å–æ–∫–∞—è/—Å—Ä–µ–¥–Ω—è—è/–Ω–∏–∑–∫–∞—è]
üî• –ü–û–ü–£–õ–Ø–†–ù–û–°–¢–¨: [–≤—ã—Å–æ–∫–∞—è/—Å—Ä–µ–¥–Ω—è—è/–Ω–∏–∑–∫–∞—è]
üìà –ü–û–¢–ï–ù–¶–ò–ê–õ –†–û–°–¢–ê: [+X%]
‚ö†Ô∏è –†–ò–°–ö –ü–ê–î–ï–ù–ò–Ø: [-Y%]
‚úÖ –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–Ø: [–ø–æ–∫—É–ø–∞—Ç—å/–¥–µ—Ä–∂–∞—Ç—å/–ø—Ä–æ–¥–∞–≤–∞—Ç—å]""",

    LlamaTaskType.RISK_ASSESSMENT: """–¢—ã - —Ä–∏—Å–∫-–º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —Ç–æ—Ä–≥–æ–≤–ª–∏ —Å–∫–∏–Ω–∞–º–∏.
–û—Ü–µ–Ω–∏–≤–∞–π —Ä–∏—Å–∫–∏:
- –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å —Ä—ã–Ω–∫–∞
- –õ–∏–∫–≤–∏–¥–Ω–æ—Å—Ç—å –ø–æ–∑–∏—Ü–∏–∏
- –ö–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—è –ø–æ—Ä—Ç—Ñ–µ–ª—è
- –í–Ω–µ—à–Ω–∏–µ —Ñ–∞–∫—Ç–æ—Ä—ã (–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∏–≥—Ä—ã, —Ç—É—Ä–Ω–∏—Ä—ã)

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞:
‚ö†Ô∏è –û–ë–©–ò–ô –£–†–û–í–ï–ù–¨ –†–ò–°–ö–ê: [1-10]
üìä –í–û–õ–ê–¢–ò–õ–¨–ù–û–°–¢–¨: [–Ω–∏–∑–∫–∞—è/—Å—Ä–µ–¥–Ω—è—è/–≤—ã—Å–æ–∫–∞—è]
üíß –õ–ò–ö–í–ò–î–ù–û–°–¢–¨: [–≤—ã—Å–æ–∫–∞—è/—Å—Ä–µ–¥–Ω—è—è/–Ω–∏–∑–∫–∞—è]
üéØ –î–ò–í–ï–†–°–ò–§–ò–ö–ê–¶–ò–Ø: [—Ö–æ—Ä–æ—à–∞—è/—Ç—Ä–µ–±—É–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è/–ø–ª–æ—Ö–∞—è]
üõ°Ô∏è –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:
- [–ø—É–Ω–∫—Ç 1]
- [–ø—É–Ω–∫—Ç 2]
- [–ø—É–Ω–∫—Ç 3]""",

    LlamaTaskType.GENERAL_CHAT: """–¢—ã - AI-–ø–æ–º–æ—â–Ω–∏–∫ –¥–ª—è DMarket Trading Bot.
–ü–æ–º–æ–≥–∞–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º:
1. –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ä—ã–Ω–æ–∫ CS:GO, Dota 2, Rust, TF2
2. –ù–∞—Ö–æ–¥–∏—Ç—å –∞—Ä–±–∏—Ç—Ä–∞–∂–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏
3. –î–∞–≤–∞—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ø–æ–∫—É–ø–∫–µ/–ø—Ä–æ–¥–∞–∂–µ
4. –û–±—ä—è—Å–Ω—è—Ç—å —Ç–æ—Ä–≥–æ–≤—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
5. –ü–æ–º–æ–≥–∞—Ç—å —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π –±–æ—Ç–∞

–ö–æ–º–∏—Å—Å–∏–∏: DMarket 7%, Waxpeer 6%, Steam 15%
–û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º, –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É.""",
}


class LlamaIntegration:
    """
    –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è Llama 3.1 8B –¥–ª—è DMarket –±–æ—Ç–∞.
    
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è:
    - Ryzen 7 5700X (8 —è–¥–µ—Ä)
    - 32 GB RAM
    - Radeon RX 6600 (8 GB VRAM)
    - Q4_K_M –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è
    """
    
    def __init__(self, config: LlamaConfig | None = None):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è."""
        self.config = config or LlamaConfig()
        self._client: httpx.AsyncClient | None = None
        self._is_available: bool | None = None
        self._last_check: datetime | None = None
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "total_tokens": 0,
            "avg_response_time_ms": 0.0,
        }
        
        logger.info(
            "llama_integration_initialized",
            model=self.config.model_name,
            url=self.config.ollama_url,
        )
    
    async def _get_client(self) -> httpx.AsyncClient:
        """–ü–æ–ª—É—á–∏—Ç—å HTTP –∫–ª–∏–µ–Ω—Ç."""
        if not HTTPX_AVAILABLE:
            raise RuntimeError("httpx –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install httpx")
        
        if self._client is None or self._client.is_closed:
            self._client = httpx.AsyncClient(timeout=self.config.timeout)
        return self._client
    
    async def check_availability(self, force: bool = False) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Ollama –∏ –º–æ–¥–µ–ª–∏.
        
        Args:
            force: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ (–∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç –∫—ç—à)
            
        Returns:
            True –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞
        """
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à –µ—Å–ª–∏ –Ω–µ –ø—Ä–æ—à–ª–æ 30 —Å–µ–∫—É–Ω–¥
        if not force and self._last_check:
            elapsed = (datetime.now() - self._last_check).total_seconds()
            if elapsed < 30 and self._is_available is not None:
                return self._is_available
        
        try:
            client = await self._get_client()
            response = await client.get(
                f"{self.config.ollama_url}/api/tags",
                timeout=5.0,
            )
            
            if response.status_code == 200:
                data = response.json()
                models = [m["name"] for m in data.get("models", [])]
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –Ω—É–∂–Ω–∞—è –º–æ–¥–µ–ª—å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞
                model_available = any(
                    self.config.model_name in m or m.startswith(self.config.model_name.split(":")[0])
                    for m in models
                )
                
                self._is_available = model_available
                self._last_check = datetime.now()
                
                if not model_available:
                    logger.warning(
                        "llama_model_not_found",
                        model=self.config.model_name,
                        available_models=models,
                    )
                
                return model_available
            
            self._is_available = False
            return False
            
        except Exception as e:
            logger.error("llama_availability_check_failed", error=str(e))
            self._is_available = False
            self._last_check = datetime.now()
            return False
    
    async def get_available_models(self) -> list[str]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π."""
        try:
            client = await self._get_client()
            response = await client.get(f"{self.config.ollama_url}/api/tags")
            
            if response.status_code == 200:
                data = response.json()
                return [m["name"] for m in data.get("models", [])]
            return []
            
        except Exception as e:
            logger.error("get_models_failed", error=str(e))
            return []
    
    def _get_system_prompt(self, task_type: LlamaTaskType) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –∑–∞–¥–∞—á–∏."""
        return TASK_PROMPTS.get(task_type, TASK_PROMPTS[LlamaTaskType.GENERAL_CHAT])
    
    async def execute_task(
        self,
        task_type: LlamaTaskType,
        user_message: str,
        context: dict[str, Any] | None = None,
        conversation_history: list[dict[str, str]] | None = None,
    ) -> LlamaResponse:
        """
        –í—ã–ø–æ–ª–Ω–∏—Ç—å –∑–∞–¥–∞—á—É —Å –ø–æ–º–æ—â—å—é Llama.
        
        Args:
            task_type: –¢–∏–ø –∑–∞–¥–∞—á–∏
            user_message: –°–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            context: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (–¥–∞–Ω–Ω—ã–µ –æ —Ä—ã–Ω–∫–µ –∏ —Ç.–¥.)
            conversation_history: –ò—Å—Ç–æ—Ä–∏—è —Ä–∞–∑–≥–æ–≤–æ—Ä–∞
            
        Returns:
            LlamaResponse —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º
        """
        start_time = datetime.now()
        self.stats["total_requests"] += 1
        
        if not await self.check_availability():
            self.stats["failed_requests"] += 1
            return LlamaResponse(
                success=False,
                response="",
                task_type=task_type,
                error="Ollama –∏–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã. –ó–∞–ø—É—Å—Ç–∏—Ç–µ: ollama serve",
            )
        
        try:
            client = await self._get_client()
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
            enhanced_message = user_message
            if context:
                context_str = json.dumps(context, ensure_ascii=False, indent=2)
                enhanced_message = f"{user_message}\n\nüìä –î–∞–Ω–Ω—ã–µ:\n```json\n{context_str}\n```"
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è
            messages = [
                {"role": "system", "content": self._get_system_prompt(task_type)},
            ]
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—Ä–∏—é —Ä–∞–∑–≥–æ–≤–æ—Ä–∞
            if conversation_history:
                messages.extend(conversation_history[-10:])  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 10 —Å–æ–æ–±—â–µ–Ω–∏–π
            
            messages.append({"role": "user", "content": enhanced_message})
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å
            response = await client.post(
                f"{self.config.ollama_url}/api/chat",
                json={
                    "model": self.config.model_name,
                    "messages": messages,
                    "stream": False,
                    "options": {
                        "temperature": self.config.temperature,
                        "top_p": self.config.top_p,
                        "num_predict": self.config.max_tokens,
                        "num_ctx": self.config.context_length,
                    },
                },
            )
            
            processing_time = (datetime.now() - start_time).total_seconds() * 1000
            
            if response.status_code == 200:
                data = response.json()
                ai_response = data.get("message", {}).get("content", "")
                tokens = data.get("eval_count", 0) + data.get("prompt_eval_count", 0)
                
                self.stats["successful_requests"] += 1
                self.stats["total_tokens"] += tokens
                
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞
                n = self.stats["successful_requests"]
                avg = self.stats["avg_response_time_ms"]
                self.stats["avg_response_time_ms"] = avg + (processing_time - avg) / n
                
                logger.info(
                    "llama_task_completed",
                    task_type=task_type.value,
                    tokens=tokens,
                    time_ms=processing_time,
                )
                
                return LlamaResponse(
                    success=True,
                    response=ai_response,
                    task_type=task_type,
                    tokens_used=tokens,
                    processing_time_ms=processing_time,
                    metadata={
                        "model": self.config.model_name,
                        "context_provided": context is not None,
                    },
                )
            else:
                self.stats["failed_requests"] += 1
                error_msg = f"HTTP {response.status_code}: {response.text[:200]}"
                logger.error("llama_request_failed", error=error_msg)
                
                return LlamaResponse(
                    success=False,
                    response="",
                    task_type=task_type,
                    processing_time_ms=processing_time,
                    error=error_msg,
                )
                
        except httpx.TimeoutException:
            self.stats["failed_requests"] += 1
            return LlamaResponse(
                success=False,
                response="",
                task_type=task_type,
                error="–¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ –∏–ª–∏ —É–º–µ–Ω—å—à–∏—Ç–µ —Ä–∞–∑–º–µ—Ä –∑–∞–ø—Ä–æ—Å–∞.",
            )
        except Exception as e:
            self.stats["failed_requests"] += 1
            logger.error("llama_task_error", error=str(e), exc_info=True)
            
            return LlamaResponse(
                success=False,
                response="",
                task_type=task_type,
                error=str(e),
            )
    
    # === –í—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–µ –º–µ—Ç–æ–¥—ã –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∑–∞–¥–∞—á ===
    
    async def analyze_market(
        self,
        game: str,
        market_data: dict[str, Any] | None = None,
    ) -> LlamaResponse:
        """
        –ê–Ω–∞–ª–∏–∑ —Ä—ã–Ω–∫–∞ –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–π –∏–≥—Ä—ã.
        
        Args:
            game: –ò–≥—Ä–∞ (csgo, dota2, rust, tf2)
            market_data: –î–∞–Ω–Ω—ã–µ –æ —Ä—ã–Ω–∫–µ (—Ü–µ–Ω—ã, –æ–±—ä–µ–º—ã –∏ —Ç.–¥.)
            
        Returns:
            LlamaResponse —Å –∞–Ω–∞–ª–∏–∑–æ–º
        """
        message = f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ç–µ–∫—É—â–∏–π —Ä—ã–Ω–æ–∫ {game.upper()}."
        return await self.execute_task(
            LlamaTaskType.MARKET_ANALYSIS,
            message,
            context=market_data,
        )
    
    async def predict_price(
        self,
        item_name: str,
        price_history: list[dict[str, Any]],
    ) -> LlamaResponse:
        """
        –ü—Ä–æ–≥–Ω–æ–∑ —Ü–µ–Ω—ã –ø—Ä–µ–¥–º–µ—Ç–∞.
        
        Args:
            item_name: –ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–º–µ—Ç–∞
            price_history: –ò—Å—Ç–æ—Ä–∏—è —Ü–µ–Ω [{date, price, volume}, ...]
            
        Returns:
            LlamaResponse —Å –ø—Ä–æ–≥–Ω–æ–∑–æ–º
        """
        message = f"–î–∞–π –ø—Ä–æ–≥–Ω–æ–∑ —Ü–µ–Ω—ã –¥–ª—è –ø—Ä–µ–¥–º–µ—Ç–∞: {item_name}"
        return await self.execute_task(
            LlamaTaskType.PRICE_PREDICTION,
            message,
            context={"item": item_name, "history": price_history},
        )
    
    async def find_arbitrage(
        self,
        opportunities: list[dict[str, Any]],
    ) -> LlamaResponse:
        """
        –ê–Ω–∞–ª–∏–∑ –∞—Ä–±–∏—Ç—Ä–∞–∂–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π.
        
        Args:
            opportunities: –°–ø–∏—Å–æ–∫ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π [{item, buy_price, sell_price, platform_buy, platform_sell}, ...]
            
        Returns:
            LlamaResponse —Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏
        """
        message = "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∞—Ä–±–∏—Ç—Ä–∞–∂–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∏ –¥–∞–π —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏."
        return await self.execute_task(
            LlamaTaskType.ARBITRAGE_RECOMMENDATION,
            message,
            context={"opportunities": opportunities},
        )
    
    async def get_trading_advice(
        self,
        portfolio: dict[str, Any],
        balance: float,
        risk_tolerance: str = "medium",
    ) -> LlamaResponse:
        """
        –¢–æ—Ä–≥–æ–≤—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏.
        
        Args:
            portfolio: –¢–µ–∫—É—â–∏–π –ø–æ—Ä—Ç—Ñ–µ–ª—å
            balance: –î–æ—Å—Ç—É–ø–Ω—ã–π –±–∞–ª–∞–Ω—Å
            risk_tolerance: –£—Ä–æ–≤–µ–Ω—å —Ä–∏—Å–∫–∞ (low, medium, high)
            
        Returns:
            LlamaResponse —Å —Å–æ–≤–µ—Ç–∞–º–∏
        """
        message = f"–î–∞–π —Ç–æ—Ä–≥–æ–≤—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏. –ú–æ–π —É—Ä–æ–≤–µ–Ω—å —Ä–∏—Å–∫–∞: {risk_tolerance}."
        return await self.execute_task(
            LlamaTaskType.TRADING_ADVICE,
            message,
            context={
                "portfolio": portfolio,
                "balance": balance,
                "risk_tolerance": risk_tolerance,
            },
        )
    
    async def evaluate_item(
        self,
        item_name: str,
        current_price: float,
        item_data: dict[str, Any] | None = None,
    ) -> LlamaResponse:
        """
        –û—Ü–µ–Ω–∫–∞ –ø—Ä–µ–¥–º–µ—Ç–∞.
        
        Args:
            item_name: –ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–º–µ—Ç–∞
            current_price: –¢–µ–∫—É—â–∞—è —Ü–µ–Ω–∞
            item_data: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ –ø—Ä–µ–¥–º–µ—Ç–µ
            
        Returns:
            LlamaResponse —Å –æ—Ü–µ–Ω–∫–æ–π
        """
        message = f"–û—Ü–µ–Ω–∏ –ø—Ä–µ–¥–º–µ—Ç: {item_name} (—Ç–µ–∫—É—â–∞—è —Ü–µ–Ω–∞: ${current_price:.2f})"
        context = {"item": item_name, "price": current_price}
        if item_data:
            context.update(item_data)
            
        return await self.execute_task(
            LlamaTaskType.ITEM_EVALUATION,
            message,
            context=context,
        )
    
    async def assess_risk(
        self,
        portfolio: dict[str, Any],
    ) -> LlamaResponse:
        """
        –û—Ü–µ–Ω–∫–∞ —Ä–∏—Å–∫–æ–≤ –ø–æ—Ä—Ç—Ñ–µ–ª—è.
        
        Args:
            portfolio: –î–∞–Ω–Ω—ã–µ –ø–æ—Ä—Ç—Ñ–µ–ª—è
            
        Returns:
            LlamaResponse —Å –æ—Ü–µ–Ω–∫–æ–π —Ä–∏—Å–∫–æ–≤
        """
        message = "–û—Ü–µ–Ω–∏ —Ä–∏—Å–∫–∏ –º–æ–µ–≥–æ –ø–æ—Ä—Ç—Ñ–µ–ª—è –∏ –¥–∞–π —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏."
        return await self.execute_task(
            LlamaTaskType.RISK_ASSESSMENT,
            message,
            context={"portfolio": portfolio},
        )
    
    async def chat(
        self,
        message: str,
        conversation_history: list[dict[str, str]] | None = None,
    ) -> LlamaResponse:
        """
        –û–±—â–∏–π —á–∞—Ç —Å AI.
        
        Args:
            message: –°–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            conversation_history: –ò—Å—Ç–æ—Ä–∏—è —Ä–∞–∑–≥–æ–≤–æ—Ä–∞
            
        Returns:
            LlamaResponse
        """
        return await self.execute_task(
            LlamaTaskType.GENERAL_CHAT,
            message,
            conversation_history=conversation_history,
        )
    
    def get_statistics(self) -> dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è."""
        return {
            **self.stats,
            "success_rate": (
                self.stats["successful_requests"] / max(1, self.stats["total_requests"])
            ) * 100,
            "model": self.config.model_name,
            "is_available": self._is_available,
        }
    
    async def close(self) -> None:
        """–ó–∞–∫—Ä—ã—Ç—å —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è."""
        if self._client and not self._client.is_closed:
            await self._client.aclose()
            self._client = None


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
_llama: LlamaIntegration | None = None


def get_llama() -> LlamaIntegration:
    """–ü–æ–ª—É—á–∏—Ç—å –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä Llama –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏."""
    global _llama
    if _llama is None:
        _llama = LlamaIntegration()
    return _llama


async def init_llama(config: LlamaConfig | None = None) -> LlamaIntegration:
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å Llama –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é."""
    global _llama
    _llama = LlamaIntegration(config)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å
    available = await _llama.check_availability()
    if available:
        logger.info("llama_ready", model=_llama.config.model_name)
    else:
        logger.warning(
            "llama_not_available",
            model=_llama.config.model_name,
            hint="–ó–∞–ø—É—Å—Ç–∏—Ç–µ: ollama serve && ollama pull llama3.1:8b",
        )
    
    return _llama
