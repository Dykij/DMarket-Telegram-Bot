"""–ú–æ–¥—É–ª—å –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ ML –º–æ–¥–µ–ª–µ–π.

–†–µ–∞–ª–∏–∑—É–µ—Ç –ª—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏ scikit-learn:
1. Cross-Validation (KFold, TimeSeriesSplit)
2. GridSearchCV –¥–ª—è –ø–æ–¥–±–æ—Ä–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
3. Pipeline –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è —É—Ç–µ—á–∫–∏ –¥–∞–Ω–Ω—ã—Ö
4. Feature Selection —Å SelectKBest
5. Model Evaluation —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏

–û—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ scikit-learn:
- https://scikit-learn.org/stable/modules/cross_validation.html
- https://scikit-learn.org/stable/modules/grid_search.html
"""

from dataclasses import dataclass, field
from datetime import datetime
from enum import StrEnum
import logging
from typing import Any

import numpy as np


logger = logging.getLogger(__name__)


class CVStrategy(StrEnum):
    """–°—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏."""

    KFOLD = "kfold"  # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è K-Fold CV
    TIME_SERIES = "time_series"  # TimeSeriesSplit –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
    STRATIFIED = "stratified"  # StratifiedKFold –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏


class ScoringMetric(StrEnum):
    """–ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π."""

    # –†–µ–≥—Ä–µ—Å—Å–∏—è
    MAE = "neg_mean_absolute_error"
    MSE = "neg_mean_squared_error"
    RMSE = "neg_root_mean_squared_error"
    R2 = "r2"

    # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
    ACCURACY = "accuracy"
    F1 = "f1"
    PRECISION = "precision"
    RECALL = "recall"


@dataclass
class TuningResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤."""

    best_params: dict[str, Any]
    best_score: float
    cv_results: dict[str, Any]
    best_estimator: Any

    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    model_name: str
    scoring: str
    cv_folds: int
    total_fits: int
    tuning_time_seconds: float
    timestamp: datetime = field(default_factory=datetime.utcnow)

    def summary(self) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å –∫—Ä–∞—Ç–∫—É—é —Å–≤–æ–¥–∫—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤."""
        return (
            f"Model: {self.model_name}\n"
            f"Best Score: {self.best_score:.4f}\n"
            f"Best Params: {self.best_params}\n"
            f"CV Folds: {self.cv_folds}\n"
            f"Total Fits: {self.total_fits}\n"
            f"Time: {self.tuning_time_seconds:.1f}s"
        )


@dataclass
class EvaluationResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏."""

    train_scores: list[float]
    test_scores: list[float]
    mean_train_score: float
    mean_test_score: float
    std_train_score: float
    std_test_score: float

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    feature_importances: dict[str, float] | None = None
    overfitting_ratio: float = 0.0  # train/test score ratio

    def is_overfitting(self, threshold: float = 0.15) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –µ—Å—Ç—å –ª–∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ."""
        if self.mean_train_score == 0:
            return False
        ratio = abs(self.mean_train_score - self.mean_test_score) / abs(self.mean_train_score)
        return ratio > threshold


class ModelTuner:
    """–ö–ª–∞—Å—Å –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ ML –º–æ–¥–µ–ª–µ–π.
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ª—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏ scikit-learn:
    - Pipeline –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–π –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
    - GridSearchCV/RandomizedSearchCV –¥–ª—è –ø–æ–¥–±–æ—Ä–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    - Cross-validation –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
    
    Example:
        >>> tuner = ModelTuner()
        >>> result = tuner.tune_random_forest(X_train, y_train)
        >>> print(result.best_params)
    """

    # –ü—Ä–µ–¥—É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –≥—Äids –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
    RANDOM_FOREST_PARAM_GRID = {
        "n_estimators": [50, 100, 200],
        "max_depth": [5, 10, 15, None],
        "min_samples_split": [2, 5, 10],
        "min_samples_leaf": [1, 2, 4],
    }

    GRADIENT_BOOSTING_PARAM_GRID = {
        "n_estimators": [50, 100, 150],
        "max_depth": [3, 5, 7],
        "learning_rate": [0.05, 0.1, 0.2],
        "min_samples_split": [2, 5, 10],
    }

    XGBOOST_PARAM_GRID = {
        "n_estimators": [50, 100, 200],
        "max_depth": [3, 5, 7],
        "learning_rate": [0.05, 0.1, 0.2],
        "subsample": [0.8, 0.9, 1.0],
        "colsample_bytree": [0.8, 0.9, 1.0],
    }

    RIDGE_PARAM_GRID = {
        "alpha": [0.1, 0.5, 1.0, 5.0, 10.0],
    }

    def __init__(
        self,
        cv_strategy: CVStrategy = CVStrategy.TIME_SERIES,
        cv_folds: int = 5,
        scoring: ScoringMetric = ScoringMetric.MAE,
        n_jobs: int = -1,
        random_state: int = 42,
    ):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç—é–Ω–µ—Ä–∞.
        
        Args:
            cv_strategy: –°—Ç—Ä–∞—Ç–µ–≥–∏—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏
            cv_folds: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–æ–ª–¥–æ–≤
            scoring: –ú–µ—Ç—Ä–∏–∫–∞ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
            n_jobs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤ (-1 = –≤—Å–µ)
            random_state: Seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
        """
        self.cv_strategy = cv_strategy
        self.cv_folds = cv_folds
        self.scoring = scoring
        self.n_jobs = n_jobs
        self.random_state = random_state

        self._sklearn_available = self._check_sklearn()

    def _check_sklearn(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å sklearn."""
        try:
            import sklearn
            return True
        except ImportError:
            logger.warning("scikit-learn not available")
            return False

    def _get_cv_splitter(self, n_samples: int) -> Any:
        """–ü–æ–ª—É—á–∏—Ç—å splitter –¥–ª—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏."""
        if not self._sklearn_available:
            return None

        from sklearn.model_selection import (
            KFold,
            StratifiedKFold,
            TimeSeriesSplit,
        )

        if self.cv_strategy == CVStrategy.TIME_SERIES:
            return TimeSeriesSplit(n_splits=self.cv_folds)
        elif self.cv_strategy == CVStrategy.STRATIFIED:
            return StratifiedKFold(
                n_splits=self.cv_folds,
                shuffle=True,
                random_state=self.random_state,
            )
        else:
            return KFold(
                n_splits=self.cv_folds,
                shuffle=True,
                random_state=self.random_state,
            )

    def create_pipeline(
        self,
        model: Any,
        use_scaling: bool = True,
        use_feature_selection: bool = False,
        n_features_to_select: int | None = None,
    ) -> Any:
        """–°–æ–∑–¥–∞—Ç—å Pipeline –¥–ª—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ –º–æ–¥–µ–ª–∏.
        
        Pipeline –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç —É—Ç–µ—á–∫—É –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏,
        –ø—Ä–∏–º–µ–Ω—è—è preprocessing —Ç–æ–ª—å–∫–æ –∫ training data –≤ –∫–∞–∂–¥–æ–º fold.
        
        Args:
            model: ML –º–æ–¥–µ–ª—å
            use_scaling: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å StandardScaler
            use_feature_selection: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å SelectKBest
            n_features_to_select: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ—Ç–±–æ—Ä–∞
            
        Returns:
            sklearn.pipeline.Pipeline
        """
        if not self._sklearn_available:
            return model

        from sklearn.feature_selection import SelectKBest, f_regression
        from sklearn.impute import SimpleImputer
        from sklearn.pipeline import Pipeline
        from sklearn.preprocessing import StandardScaler

        steps = []

        # 1. Imputer –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ NaN
        steps.append(("imputer", SimpleImputer(strategy="median")))

        # 2. Scaling (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        if use_scaling:
            steps.append(("scaler", StandardScaler()))

        # 3. Feature Selection (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        if use_feature_selection and n_features_to_select:
            steps.append((
                "feature_selection",
                SelectKBest(f_regression, k=n_features_to_select)
            ))

        # 4. –ú–æ–¥–µ–ª—å
        steps.append(("model", model))

        return Pipeline(steps)

    def tune_random_forest(
        self,
        X: np.ndarray,
        y: np.ndarray,
        param_grid: dict[str, list] | None = None,
        use_randomized: bool = False,
        n_iter: int = 50,
    ) -> TuningResult:
        """–ù–∞—Å—Ç—Ä–æ–∏—Ç—å RandomForestRegressor.
        
        Args:
            X: –ü—Ä–∏–∑–Ω–∞–∫–∏
            y: –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è
            param_grid: –°–µ—Ç–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –ø—Ä–µ–¥—É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω–∞—è)
            use_randomized: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å RandomizedSearchCV (–±—ã—Å—Ç—Ä–µ–µ)
            n_iter: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π –¥–ª—è RandomizedSearchCV
            
        Returns:
            TuningResult —Å –ª—É—á—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        """
        if not self._sklearn_available:
            return self._fallback_result("RandomForest")

        from sklearn.ensemble import RandomForestRegressor

        model = RandomForestRegressor(random_state=self.random_state, n_jobs=-1)
        grid = param_grid or self.RANDOM_FOREST_PARAM_GRID

        return self._run_grid_search(
            model=model,
            param_grid=grid,
            X=X,
            y=y,
            model_name="RandomForestRegressor",
            use_randomized=use_randomized,
            n_iter=n_iter,
        )

    def tune_gradient_boosting(
        self,
        X: np.ndarray,
        y: np.ndarray,
        param_grid: dict[str, list] | None = None,
        use_randomized: bool = False,
        n_iter: int = 50,
    ) -> TuningResult:
        """–ù–∞—Å—Ç—Ä–æ–∏—Ç—å GradientBoostingRegressor."""
        if not self._sklearn_available:
            return self._fallback_result("GradientBoosting")

        from sklearn.ensemble import GradientBoostingRegressor

        model = GradientBoostingRegressor(random_state=self.random_state)
        grid = param_grid or self.GRADIENT_BOOSTING_PARAM_GRID

        return self._run_grid_search(
            model=model,
            param_grid=grid,
            X=X,
            y=y,
            model_name="GradientBoostingRegressor",
            use_randomized=use_randomized,
            n_iter=n_iter,
        )

    def tune_xgboost(
        self,
        X: np.ndarray,
        y: np.ndarray,
        param_grid: dict[str, list] | None = None,
        use_randomized: bool = True,
        n_iter: int = 50,
    ) -> TuningResult:
        """–ù–∞—Å—Ç—Ä–æ–∏—Ç—å XGBRegressor (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)."""
        try:
            from xgboost import XGBRegressor
        except ImportError:
            logger.warning("XGBoost not available")
            return self._fallback_result("XGBoost")

        model = XGBRegressor(
            random_state=self.random_state,
            n_jobs=-1,
            objective="reg:squarederror",
        )
        grid = param_grid or self.XGBOOST_PARAM_GRID

        return self._run_grid_search(
            model=model,
            param_grid=grid,
            X=X,
            y=y,
            model_name="XGBRegressor",
            use_randomized=use_randomized,
            n_iter=n_iter,
        )

    def tune_ridge(
        self,
        X: np.ndarray,
        y: np.ndarray,
        param_grid: dict[str, list] | None = None,
    ) -> TuningResult:
        """–ù–∞—Å—Ç—Ä–æ–∏—Ç—å Ridge Regression."""
        if not self._sklearn_available:
            return self._fallback_result("Ridge")

        from sklearn.linear_model import Ridge

        model = Ridge()
        grid = param_grid or self.RIDGE_PARAM_GRID

        return self._run_grid_search(
            model=model,
            param_grid=grid,
            X=X,
            y=y,
            model_name="Ridge",
            use_randomized=False,
            n_iter=10,
        )

    def _run_grid_search(
        self,
        model: Any,
        param_grid: dict[str, list],
        X: np.ndarray,
        y: np.ndarray,
        model_name: str,
        use_randomized: bool = False,
        n_iter: int = 50,
    ) -> TuningResult:
        """–ó–∞–ø—É—Å—Ç–∏—Ç—å GridSearchCV –∏–ª–∏ RandomizedSearchCV."""
        import time

        from sklearn.model_selection import GridSearchCV, RandomizedSearchCV

        cv = self._get_cv_splitter(len(X))

        # –°–æ–∑–¥–∞—ë–º pipeline
        pipeline = self.create_pipeline(model)

        # –ê–¥–∞–ø—Ç–∏—Ä—É–µ–º param_grid –¥–ª—è pipeline
        pipeline_param_grid = {
            f"model__{k}": v for k, v in param_grid.items()
        }

        start_time = time.time()

        if use_randomized:
            search = RandomizedSearchCV(
                estimator=pipeline,
                param_distributions=pipeline_param_grid,
                n_iter=n_iter,
                cv=cv,
                scoring=self.scoring.value,
                n_jobs=self.n_jobs,
                random_state=self.random_state,
                return_train_score=True,
            )
        else:
            search = GridSearchCV(
                estimator=pipeline,
                param_grid=pipeline_param_grid,
                cv=cv,
                scoring=self.scoring.value,
                n_jobs=self.n_jobs,
                return_train_score=True,
            )

        try:
            search.fit(X, y)
            elapsed = time.time() - start_time

            # –£–±–∏—Ä–∞–µ–º –ø—Ä–µ—Ñ–∏–∫—Å "model__" –∏–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            best_params = {
                k.replace("model__", ""): v
                for k, v in search.best_params_.items()
            }

            return TuningResult(
                best_params=best_params,
                best_score=-search.best_score_ if "neg" in self.scoring.value else search.best_score_,
                cv_results=dict(search.cv_results_),
                best_estimator=search.best_estimator_,
                model_name=model_name,
                scoring=self.scoring.value,
                cv_folds=self.cv_folds,
                total_fits=len(search.cv_results_["mean_test_score"]) * self.cv_folds,
                tuning_time_seconds=elapsed,
            )

        except Exception as e:
            logger.error(f"Grid search failed: {e}")
            return self._fallback_result(model_name)

    def _fallback_result(self, model_name: str) -> TuningResult:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –ø—Ä–∏ –æ—à–∏–±–∫–µ."""
        return TuningResult(
            best_params={},
            best_score=0.0,
            cv_results={},
            best_estimator=None,
            model_name=model_name,
            scoring=self.scoring.value,
            cv_folds=self.cv_folds,
            total_fits=0,
            tuning_time_seconds=0.0,
        )

    def evaluate_model(
        self,
        model: Any,
        X: np.ndarray,
        y: np.ndarray,
        feature_names: list[str] | None = None,
    ) -> EvaluationResult:
        """–û—Ü–µ–Ω–∏—Ç—å –º–æ–¥–µ–ª—å —Å –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–µ–π.
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç cross_val_score –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è train –∏ test scores
        –Ω–∞ –∫–∞–∂–¥–æ–º fold, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ü–µ–Ω–∏—Ç—å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ.
        
        Args:
            model: –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –∏–ª–∏ pipeline
            X: –ü—Ä–∏–∑–Ω–∞–∫–∏
            y: –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è
            feature_names: –ù–∞–∑–≤–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è feature importance
            
        Returns:
            EvaluationResult —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
        """
        if not self._sklearn_available:
            return EvaluationResult(
                train_scores=[],
                test_scores=[],
                mean_train_score=0.0,
                mean_test_score=0.0,
                std_train_score=0.0,
                std_test_score=0.0,
            )

        from sklearn.model_selection import cross_validate

        cv = self._get_cv_splitter(len(X))

        results = cross_validate(
            model,
            X,
            y,
            cv=cv,
            scoring=self.scoring.value,
            return_train_score=True,
            n_jobs=self.n_jobs,
        )

        train_scores = results["train_score"].tolist()
        test_scores = results["test_score"].tolist()

        # Feature importances (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ)
        feature_importances = None
        if hasattr(model, "feature_importances_") and feature_names:
            importances = model.feature_importances_
            feature_importances = dict(zip(feature_names, importances))
        elif hasattr(model, "named_steps"):
            # Pipeline case
            final_model = model.named_steps.get("model")
            if hasattr(final_model, "feature_importances_") and feature_names:
                importances = final_model.feature_importances_
                feature_importances = dict(zip(feature_names, importances))

        mean_train = float(np.mean(train_scores))
        mean_test = float(np.mean(test_scores))

        return EvaluationResult(
            train_scores=train_scores,
            test_scores=test_scores,
            mean_train_score=abs(mean_train) if "neg" in self.scoring.value else mean_train,
            mean_test_score=abs(mean_test) if "neg" in self.scoring.value else mean_test,
            std_train_score=float(np.std(train_scores)),
            std_test_score=float(np.std(test_scores)),
            feature_importances=feature_importances,
            overfitting_ratio=abs(mean_train - mean_test) / abs(mean_train) if mean_train != 0 else 0.0,
        )

    def compare_models(
        self,
        X: np.ndarray,
        y: np.ndarray,
        models: list[str] | None = None,
    ) -> dict[str, EvaluationResult]:
        """–°—Ä–∞–≤–Ω–∏—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª–µ–π.
        
        Args:
            X: –ü—Ä–∏–∑–Ω–∞–∫–∏
            y: –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è
            models: –°–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –º–æ–¥–µ–ª–µ–π (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –≤—Å–µ)
            
        Returns:
            Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
        """
        models = models or ["random_forest", "gradient_boosting", "ridge"]
        results = {}

        model_classes = {
            "random_forest": self._create_random_forest,
            "gradient_boosting": self._create_gradient_boosting,
            "ridge": self._create_ridge,
            "xgboost": self._create_xgboost,
        }

        for model_name in models:
            if model_name not in model_classes:
                continue

            try:
                model = model_classes[model_name]()
                if model is not None:
                    pipeline = self.create_pipeline(model)
                    pipeline.fit(X, y)
                    results[model_name] = self.evaluate_model(pipeline, X, y)
            except Exception as e:
                logger.warning(f"Failed to evaluate {model_name}: {e}")

        return results

    def _create_random_forest(self) -> Any:
        """–°–æ–∑–¥–∞—Ç—å RandomForest —Å –±–∞–∑–æ–≤—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏."""
        if not self._sklearn_available:
            return None
        from sklearn.ensemble import RandomForestRegressor
        return RandomForestRegressor(
            n_estimators=100,
            max_depth=10,
            random_state=self.random_state,
            n_jobs=-1,
        )

    def _create_gradient_boosting(self) -> Any:
        """–°–æ–∑–¥–∞—Ç—å GradientBoosting —Å –±–∞–∑–æ–≤—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏."""
        if not self._sklearn_available:
            return None
        from sklearn.ensemble import GradientBoostingRegressor
        return GradientBoostingRegressor(
            n_estimators=100,
            max_depth=5,
            learning_rate=0.1,
            random_state=self.random_state,
        )

    def _create_ridge(self) -> Any:
        """–°–æ–∑–¥–∞—Ç—å Ridge —Å –±–∞–∑–æ–≤—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏."""
        if not self._sklearn_available:
            return None
        from sklearn.linear_model import Ridge
        return Ridge(alpha=1.0)

    def _create_xgboost(self) -> Any:
        """–°–æ–∑–¥–∞—Ç—å XGBoost —Å –±–∞–∑–æ–≤—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏."""
        try:
            from xgboost import XGBRegressor
            return XGBRegressor(
                n_estimators=100,
                max_depth=5,
                learning_rate=0.1,
                random_state=self.random_state,
                n_jobs=-1,
            )
        except ImportError:
            return None


class AutoMLSelector:
    """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏.
    
    –°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª–µ–π, —Ç—é–Ω–∏—Ç –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    –∏ –≤—ã–±–∏—Ä–∞–µ—Ç –ª—É—á—à—É—é –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏.
    
    Example:
        >>> selector = AutoMLSelector()
        >>> best_model, results = selector.select_best_model(X, y)
        >>> print(results.summary())
    """

    def __init__(
        self,
        cv_folds: int = 5,
        scoring: ScoringMetric = ScoringMetric.MAE,
        time_budget_seconds: int = 300,
    ):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è AutoML.
        
        Args:
            cv_folds: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–æ–ª–¥–æ–≤ CV
            scoring: –ú–µ—Ç—Ä–∏–∫–∞ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
            time_budget_seconds: –í—Ä–µ–º–µ–Ω–Ω–æ–π –±—é–¥–∂–µ—Ç (—Å–µ–∫—É–Ω–¥)
        """
        self.cv_folds = cv_folds
        self.scoring = scoring
        self.time_budget_seconds = time_budget_seconds

        self.tuner = ModelTuner(
            cv_strategy=CVStrategy.TIME_SERIES,
            cv_folds=cv_folds,
            scoring=scoring,
        )

    def select_best_model(
        self,
        X: np.ndarray,
        y: np.ndarray,
        include_xgboost: bool = True,
    ) -> tuple[Any, dict[str, TuningResult]]:
        """–í—ã–±—Ä–∞—Ç—å –ª—É—á—à—É—é –º–æ–¥–µ–ª—å.
        
        Args:
            X: –ü—Ä–∏–∑–Ω–∞–∫–∏
            y: –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è
            include_xgboost: –í–∫–ª—é—á–∏—Ç—å XGBoost –≤ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
            
        Returns:
            Tuple (–ª—É—á—à–∞—è –º–æ–¥–µ–ª—å, dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π)
        """
        import time

        results = {}
        start_time = time.time()
        time_per_model = self.time_budget_seconds // 4

        # 1. RandomForest
        if time.time() - start_time < self.time_budget_seconds:
            logger.info("Tuning RandomForest...")
            results["random_forest"] = self.tuner.tune_random_forest(
                X, y, use_randomized=True, n_iter=30
            )

        # 2. GradientBoosting
        if time.time() - start_time < self.time_budget_seconds:
            logger.info("Tuning GradientBoosting...")
            results["gradient_boosting"] = self.tuner.tune_gradient_boosting(
                X, y, use_randomized=True, n_iter=30
            )

        # 3. XGBoost (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        if include_xgboost and time.time() - start_time < self.time_budget_seconds:
            logger.info("Tuning XGBoost...")
            results["xgboost"] = self.tuner.tune_xgboost(
                X, y, use_randomized=True, n_iter=30
            )

        # 4. Ridge (–±—ã—Å—Ç—Ä–æ)
        if time.time() - start_time < self.time_budget_seconds:
            logger.info("Tuning Ridge...")
            results["ridge"] = self.tuner.tune_ridge(X, y)

        # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
        best_model = None
        best_score = float("inf") if "neg" in self.scoring.value else float("-inf")

        for name, result in results.items():
            if result.best_estimator is None:
                continue

            is_better = (
                result.best_score < best_score
                if "neg" in self.scoring.value
                else result.best_score > best_score
            )

            if is_better:
                best_score = result.best_score
                best_model = result.best_estimator

        return best_model, results

    def get_recommendations(
        self,
        results: dict[str, TuningResult],
    ) -> list[str]:
        """–ü–æ–ª—É—á–∏—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.
        
        Args:
            results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        """
        recommendations = []

        if not results:
            recommendations.append("No results available. Check if sklearn is installed.")
            return recommendations

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ score
        sorted_results = sorted(
            [(name, r) for name, r in results.items() if r.best_estimator is not None],
            key=lambda x: x[1].best_score,
            reverse="neg" not in self.scoring.value,
        )

        if not sorted_results:
            recommendations.append("No models were successfully trained.")
            return recommendations

        best_name, best_result = sorted_results[0]

        recommendations.append(
            f"‚úÖ Best model: {best_name} (Score: {best_result.best_score:.4f})"
        )

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–Ω–∏—Ü—É –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏
        if len(sorted_results) >= 2:
            second_name, second_result = sorted_results[1]
            diff = abs(best_result.best_score - second_result.best_score)
            if diff < 0.01:
                recommendations.append(
                    f"‚ö†Ô∏è {second_name} is very close. Consider ensemble."
                )

        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –º–æ–¥–µ–ª—è–º
        if best_name == "random_forest":
            recommendations.append("üí° RandomForest is robust to outliers.")
        elif best_name == "gradient_boosting":
            recommendations.append("üí° GradientBoosting may overfit. Monitor train/test gap.")
        elif best_name == "xgboost":
            recommendations.append("üí° XGBoost is fast. Consider early stopping in production.")
        elif best_name == "ridge":
            recommendations.append("üí° Ridge is simple but may underfit complex patterns.")

        return recommendations
