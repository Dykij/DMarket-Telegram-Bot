"""
AI Integration Handler –¥–ª—è Telegram –±–æ—Ç–∞.

–ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –ª–æ–∫–∞–ª—å–Ω—ã–µ LLM (Ollama) –∏ MCP —Å–µ—Ä–≤–µ—Ä –¥–ª—è AI-–ø–æ–º–æ—â–Ω–∏–∫–∞.
–ü–æ–¥–¥–µ—Ä–∂–∫–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π: Llama 3.1, Qwen 2.5, Mistral, Gemma 2.
"""

import asyncio
import json
from datetime import datetime
from enum import Enum
from typing import Any

import structlog
from telegram import InlineKeyboardButton, InlineKeyboardMarkup, Update
from telegram.ext import ContextTypes

try:
    import httpx

    HTTPX_AVAILABLE = True
except ImportError:
    HTTPX_AVAILABLE = False


logger = structlog.get_logger(__name__)


class AIModel(str, Enum):
    """–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ AI –º–æ–¥–µ–ª–∏."""

    LLAMA_31_8B = "llama3.1:8b"
    LLAMA_31_70B = "llama3.1:70b"
    QWEN_25_7B = "qwen2.5:7b"
    QWEN_25_14B = "qwen2.5:14b"
    MISTRAL_7B = "mistral:7b"
    GEMMA2_9B = "gemma2:9b"
    CODELLAMA_13B = "codellama:13b"


# –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –º–æ–¥–µ–ª—è–º –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á
MODEL_RECOMMENDATIONS = {
    "general_chat": {
        "model": AIModel.LLAMA_31_8B,
        "reason": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —á–∞—Ç–∞, —Ö–æ—Ä–æ—à–∏–π –±–∞–ª–∞–Ω—Å —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–∞",
        "vram_required": "6-8 GB",
        "tokens_per_sec_cpu": "20-40",
    },
    "market_analysis": {
        "model": AIModel.QWEN_25_7B,
        "reason": "–û—Ç–ª–∏—á–Ω—ã–µ –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏, –ø–æ–¥–¥–µ—Ä–∂–∫–∞ —á–∏—Å–µ–ª –∏ –¥–∞–Ω–Ω—ã—Ö",
        "vram_required": "5-7 GB",
        "tokens_per_sec_cpu": "25-45",
    },
    "trading_advice": {
        "model": AIModel.MISTRAL_7B,
        "reason": "–ë—ã—Å—Ç—Ä—ã–µ –æ—Ç–≤–µ—Ç—ã, —Ö–æ—Ä–æ—à–∞—è –ª–æ–≥–∏–∫–∞ –¥–ª—è —Ç–æ—Ä–≥–æ–≤—ã—Ö —Ä–µ—à–µ–Ω–∏–π",
        "vram_required": "5-6 GB",
        "tokens_per_sec_cpu": "30-50",
    },
    "coding_automation": {
        "model": AIModel.CODELLAMA_13B,
        "reason": "–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è –∫–æ–¥–∞ –∏ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏",
        "vram_required": "10-12 GB",
        "tokens_per_sec_cpu": "15-25",
    },
}

# –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è DMarket –±–æ—Ç–∞
DMARKET_SYSTEM_PROMPT = """–¢—ã - AI-–ø–æ–º–æ—â–Ω–∏–∫ –¥–ª—è DMarket Trading Bot. 
–¢—ã –ø–æ–º–æ–≥–∞–µ—à—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º:
1. –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ä—ã–Ω–æ–∫ CS:GO, Dota 2, Rust, TF2
2. –ù–∞—Ö–æ–¥–∏—Ç—å –∞—Ä–±–∏—Ç—Ä–∞–∂–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏
3. –î–∞–≤–∞—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ø–æ–∫—É–ø–∫–µ/–ø—Ä–æ–¥–∞–∂–µ
4. –û–±—ä—è—Å–Ω—è—Ç—å —Ç–æ—Ä–≥–æ–≤—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
5. –ü–æ–º–æ–≥–∞—Ç—å —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π –±–æ—Ç–∞

–ö–æ–º–∏—Å—Å–∏–∏ –ø–ª–æ—â–∞–¥–æ–∫:
- DMarket: 7%
- Waxpeer: 6% 
- Steam Market: 15%

–£—Ä–æ–≤–Ω–∏ –∞—Ä–±–∏—Ç—Ä–∞–∂–∞:
- boost: $0.50-$3 (–Ω–∞—á–∏–Ω–∞—é—â–∏–µ)
- standard: $3-$10 (—Å—Ç–∞–Ω–¥–∞—Ä—Ç)
- medium: $10-$30 (—Å—Ä–µ–¥–Ω–∏–π)
- advanced: $30-$100 (–ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π)
- pro: $100+ (–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª)

–û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É.
–ï—Å–ª–∏ –Ω—É–∂–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ü–µ–Ω–∞—Ö –∏–ª–∏ –ø—Ä–µ–¥–º–µ—Ç–∞—Ö - –∏—Å–ø–æ–ª—å–∑—É–π –¥–æ—Å—Ç—É–ø–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã."""


class AIIntegrationHandler:
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ AI –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏."""

    def __init__(
        self,
        ollama_url: str = "http://localhost:11434",
        default_model: str = "llama3.1:8b",
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è AI –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞.

        Args:
            ollama_url: URL Ollama —Å–µ—Ä–≤–µ—Ä–∞
            default_model: –ú–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        """
        self.ollama_url = ollama_url
        self.default_model = default_model
        self.conversation_history: dict[int, list[dict]] = {}
        self.user_models: dict[int, str] = {}
        self._mcp_server = None

    async def check_ollama_status(self) -> dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—Ç–∞—Ç—É—Å Ollama —Å–µ—Ä–≤–µ—Ä–∞."""
        if not HTTPX_AVAILABLE:
            return {"available": False, "error": "httpx not installed"}

        try:
            async with httpx.AsyncClient(timeout=5.0) as client:
                response = await client.get(f"{self.ollama_url}/api/tags")
                if response.status_code == 200:
                    data = response.json()
                    models = [m["name"] for m in data.get("models", [])]
                    return {
                        "available": True,
                        "models": models,
                        "url": self.ollama_url,
                    }
                return {"available": False, "error": f"HTTP {response.status_code}"}
        except Exception as e:
            return {"available": False, "error": str(e)}

    async def list_available_models(self) -> list[str]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π."""
        status = await self.check_ollama_status()
        return status.get("models", [])

    async def chat_with_ai(
        self,
        user_id: int,
        message: str,
        model: str | None = None,
        context: dict[str, Any] | None = None,
    ) -> str:
        """
        –û—Ç–ø—Ä–∞–≤–∏—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ AI.

        Args:
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            message: –°–æ–æ–±—â–µ–Ω–∏–µ
            model: –ú–æ–¥–µ–ª—å (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            context: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

        Returns:
            –û—Ç–≤–µ—Ç AI
        """
        if not HTTPX_AVAILABLE:
            return "‚ùå httpx –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install httpx"

        model = model or self.user_models.get(user_id) or self.default_model

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏—Å—Ç–æ—Ä–∏–∏ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞
        if user_id not in self.conversation_history:
            self.conversation_history[user_id] = []

        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –µ—Å–ª–∏ –µ—Å—Ç—å
        enhanced_message = message
        if context:
            context_str = json.dumps(context, ensure_ascii=False, indent=2)
            enhanced_message = f"{message}\n\n–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n{context_str}"

        # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        self.conversation_history[user_id].append({
            "role": "user",
            "content": enhanced_message,
        })

        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –ø–æ—Å–ª–µ–¥–Ω–∏–º–∏ 10 —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏
        if len(self.conversation_history[user_id]) > 20:
            self.conversation_history[user_id] = self.conversation_history[user_id][-20:]

        try:
            async with httpx.AsyncClient(timeout=120.0) as client:
                response = await client.post(
                    f"{self.ollama_url}/api/chat",
                    json={
                        "model": model,
                        "messages": [
                            {"role": "system", "content": DMARKET_SYSTEM_PROMPT},
                            *self.conversation_history[user_id],
                        ],
                        "stream": False,
                        "options": {
                            "temperature": 0.7,
                            "top_p": 0.9,
                            "num_predict": 1024,
                        },
                    },
                )

                if response.status_code == 200:
                    data = response.json()
                    ai_response = data.get("message", {}).get("content", "–ù–µ—Ç –æ—Ç–≤–µ—Ç–∞")

                    # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç –≤ –∏—Å—Ç–æ—Ä–∏—é
                    self.conversation_history[user_id].append({
                        "role": "assistant",
                        "content": ai_response,
                    })

                    return ai_response
                else:
                    return f"‚ùå –û—à–∏–±–∫–∞ Ollama: HTTP {response.status_code}"

        except httpx.TimeoutException:
            return "‚ùå –¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ –∫ Ollama. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
        except Exception as e:
            logger.error("ai_chat_error", error=str(e), exc_info=True)
            return f"‚ùå –û—à–∏–±–∫–∞: {str(e)}"

    def clear_history(self, user_id: int) -> None:
        """–û—á–∏—Å—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é —Ä–∞–∑–≥–æ–≤–æ—Ä–∞."""
        if user_id in self.conversation_history:
            self.conversation_history[user_id] = []

    def set_user_model(self, user_id: int, model: str) -> None:
        """–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è."""
        self.user_models[user_id] = model


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
_ai_handler: AIIntegrationHandler | None = None


def get_ai_handler() -> AIIntegrationHandler:
    """–ü–æ–ª—É—á–∏—Ç—å AI handler."""
    global _ai_handler
    if _ai_handler is None:
        _ai_handler = AIIntegrationHandler()
    return _ai_handler


# === Telegram Handlers ===


async def ai_command(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /ai - –≥–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é AI."""
    if not update.message:
        return

    handler = get_ai_handler()
    status = await handler.check_ollama_status()

    keyboard = [
        [
            InlineKeyboardButton("üí¨ –ß–∞—Ç —Å AI", callback_data="ai_chat"),
            InlineKeyboardButton("üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∏", callback_data="ai_settings"),
        ],
        [
            InlineKeyboardButton("üìä –ê–Ω–∞–ª–∏–∑ —Ä—ã–Ω–∫–∞", callback_data="ai_analyze_market"),
            InlineKeyboardButton("üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏", callback_data="ai_recommendations"),
        ],
        [
            InlineKeyboardButton("üìã –°—Ç–∞—Ç—É—Å", callback_data="ai_status"),
            InlineKeyboardButton("üîÑ –û—á–∏—Å—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é", callback_data="ai_clear"),
        ],
    ]

    status_emoji = "‚úÖ" if status.get("available") else "‚ùå"
    models_count = len(status.get("models", []))

    text = f"""ü§ñ **AI –ü–æ–º–æ—â–Ω–∏–∫ DMarket Bot**

**–°—Ç–∞—Ç—É—Å Ollama:** {status_emoji}
**–ú–æ–¥–µ–ª–µ–π –¥–æ—Å—Ç—É–ø–Ω–æ:** {models_count}
**URL:** `{handler.ollama_url}`

**–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:**
‚Ä¢ üí¨ –ß–∞—Ç –æ —Ç—Ä–µ–π–¥–∏–Ω–≥–µ –∏ —Ä—ã–Ω–∫–µ
‚Ä¢ üìä –ê–Ω–∞–ª–∏–∑ —Ä—ã–Ω–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
‚Ä¢ üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∞—Ä–±–∏—Ç—Ä–∞–∂—É
‚Ä¢ üîÆ –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç—Ä–µ–Ω–¥–æ–≤

–í—ã–±–µ—Ä–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ:"""

    await update.message.reply_text(
        text,
        parse_mode="Markdown",
        reply_markup=InlineKeyboardMarkup(keyboard),
    )


async def ai_chat_command(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /ai_chat - —á–∞—Ç —Å AI."""
    if not update.message:
        return

    # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ –∫–æ–º–∞–Ω–¥—ã
    if context.args:
        message = " ".join(context.args)
    else:
        await update.message.reply_text(
            "üí¨ **AI –ß–∞—Ç**\n\n"
            "–û—Ç–ø—Ä–∞–≤—å—Ç–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ—Å–ª–µ –∫–æ–º–∞–Ω–¥—ã:\n"
            "`/ai_chat –ö–∞–∫–∏–µ —Å–µ–π—á–∞—Å –ª—É—á—à–∏–µ –∞—Ä–±–∏—Ç—Ä–∞–∂–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏?`\n\n"
            "–ò–ª–∏ –ø—Ä–æ—Å—Ç–æ –Ω–∞–ø–∏—à–∏—Ç–µ –≤–æ–ø—Ä–æ—Å:",
            parse_mode="Markdown",
        )
        return

    handler = get_ai_handler()
    user_id = update.effective_user.id

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —á—Ç–æ –ø–µ—á–∞—Ç–∞–µ–º
    await update.message.chat.send_action("typing")

    response = await handler.chat_with_ai(user_id, message)

    await update.message.reply_text(
        f"ü§ñ **AI:**\n\n{response}",
        parse_mode="Markdown",
    )


async def ai_models_command(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /ai_models - —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π."""
    if not update.message:
        return

    handler = get_ai_handler()
    models = await handler.list_available_models()

    if not models:
        await update.message.reply_text(
            "‚ùå **Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –∏–ª–∏ –Ω–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π**\n\n"
            "–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –º–æ–¥–µ–ª—å:\n"
            "```bash\n"
            "ollama pull llama3.1:8b\n"
            "```",
            parse_mode="Markdown",
        )
        return

    text = "üìã **–î–æ—Å—Ç—É–ø–Ω—ã–µ AI –º–æ–¥–µ–ª–∏:**\n\n"
    for model in models:
        text += f"‚Ä¢ `{model}`\n"

    text += "\n**–†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –≤–∞—à–µ–≥–æ –∂–µ–ª–µ–∑–∞:**\n"
    text += "(Ryzen 7 5700X, 32GB RAM, RX 6600 8GB VRAM)\n\n"

    text += "üèÜ **–õ—É—á—à–∏–π –≤—ã–±–æ—Ä:** `llama3.1:8b` –∏–ª–∏ `qwen2.5:7b`\n"
    text += "‚ö° **–î–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏:** `mistral:7b`\n"
    text += "üìä **–î–ª—è –∞–Ω–∞–ª–∏–∑–∞:** `qwen2.5:14b` (Q4 –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è)\n\n"

    text += "–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –º–æ–¥–µ–ª—å:\n"
    text += "`/ai_set_model <model_name>`"

    await update.message.reply_text(text, parse_mode="Markdown")


async def ai_set_model_command(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /ai_set_model - —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –º–æ–¥–µ–ª—å."""
    if not update.message:
        return

    if not context.args:
        await update.message.reply_text(
            "‚ùå –£–∫–∞–∂–∏—Ç–µ –º–æ–¥–µ–ª—å: `/ai_set_model llama3.1:8b`",
            parse_mode="Markdown",
        )
        return

    model = context.args[0]
    handler = get_ai_handler()
    user_id = update.effective_user.id

    handler.set_user_model(user_id, model)

    await update.message.reply_text(
        f"‚úÖ –ú–æ–¥–µ–ª—å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞: `{model}`",
        parse_mode="Markdown",
    )


async def ai_analyze_command(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /ai_analyze - –∞–Ω–∞–ª–∏–∑ —Ä—ã–Ω–∫–∞ —Å AI."""
    if not update.message:
        return

    # –ü–æ–ª—É—á–∞–µ–º –∏–≥—Ä—É –∏–∑ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤
    game = context.args[0] if context.args else "csgo"

    await update.message.chat.send_action("typing")

    handler = get_ai_handler()
    user_id = update.effective_user.id

    # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –∫ AI
    prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ç–µ–∫—É—â—É—é —Å–∏—Ç—É–∞—Ü–∏—é –Ω–∞ —Ä—ã–Ω–∫–µ {game.upper()}:
1. –û–±—â–∏–µ —Ç—Ä–µ–Ω–¥—ã
2. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ø–æ–∫—É–ø–∫–µ/–ø—Ä–æ–¥–∞–∂–µ
3. –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –∞—Ä–±–∏—Ç—Ä–∞–∂–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏
4. –†–∏—Å–∫–∏

–î–∞–π –∫—Ä–∞—Ç–∫–∏–π –∞–Ω–∞–ª–∏–∑ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–≤–æ–∏—Ö –∑–Ω–∞–Ω–∏–π –æ —Ä—ã–Ω–∫–µ —Å–∫–∏–Ω–æ–≤."""

    response = await handler.chat_with_ai(user_id, prompt)

    await update.message.reply_text(
        f"üìä **AI –ê–Ω–∞–ª–∏–∑ —Ä—ã–Ω–∫–∞ {game.upper()}:**\n\n{response}",
        parse_mode="Markdown",
    )


async def ai_recommend_command(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /ai_recommend - —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏."""
    if not update.message:
        return

    text = """üß† **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –≤—ã–±–æ—Ä—É AI –º–æ–¥–µ–ª–∏**

**–í–∞—à–µ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ:**
‚Ä¢ CPU: Ryzen 7 5700X (8 —è–¥–µ—Ä, 16 –ø–æ—Ç–æ–∫–æ–≤)
‚Ä¢ RAM: 32 –ì–ë
‚Ä¢ GPU: Radeon RX 6600 (8 –ì–ë VRAM)

**–õ—É—á—à–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞:**

ü•á **Llama 3.1 8B** (–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
‚Ä¢ VRAM: 6-8 GB (Q4 –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è)
‚Ä¢ CPU: 20-40 —Ç–æ–∫–µ–Ω–æ–≤/—Å
‚Ä¢ –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è, —Ö–æ—Ä–æ—à–æ –≥–æ–≤–æ—Ä–∏—Ç –ø–æ-—Ä—É—Å—Å–∫–∏

ü•à **Qwen 2.5 7B** (–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞)
‚Ä¢ VRAM: 5-7 GB
‚Ä¢ CPU: 25-45 —Ç–æ–∫–µ–Ω–æ–≤/—Å
‚Ä¢ –û—Ç–ª–∏—á–Ω–∞—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö

ü•â **Mistral 7B** (–î–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏)
‚Ä¢ VRAM: 5-6 GB
‚Ä¢ CPU: 30-50 —Ç–æ–∫–µ–Ω–æ–≤/—Å
‚Ä¢ –ë—ã—Å—Ç—Ä—ã–µ –æ—Ç–≤–µ—Ç—ã

**–£—Å—Ç–∞–Ω–æ–≤–∫–∞ —á–µ—Ä–µ–∑ Ollama:**
```bash
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Ollama (Linux)
curl -fsSL https://ollama.com/install.sh | sh

# –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
ollama pull llama3.1:8b

# –ó–∞–ø—É—Å–∫
ollama serve
```

**–î–ª—è AMD RX 6600:**
```bash
# Ubuntu —Å ROCm
HSA_OVERRIDE_GFX_VERSION=10.3.0 ollama serve
```

**–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã:**
‚Ä¢ **LM Studio** - GUI –¥–ª—è Windows, –ø–æ–¥–¥–µ—Ä–∂–∫–∞ Vulkan
‚Ä¢ **llama.cpp** - OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π API
‚Ä¢ **LocalAI** - –ø–æ–ª–Ω–∞—è —ç–º—É–ª—è—Ü–∏—è OpenAI API"""

    await update.message.reply_text(text, parse_mode="Markdown")


async def ai_status_callback(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Callback –¥–ª—è —Å—Ç–∞—Ç—É—Å–∞ AI."""
    query = update.callback_query
    if not query:
        return

    await query.answer()

    handler = get_ai_handler()
    status = await handler.check_ollama_status()

    if status.get("available"):
        models = status.get("models", [])
        text = f"""‚úÖ **Ollama –¥–æ—Å—Ç—É–ø–Ω–∞**

**URL:** `{handler.ollama_url}`
**–ú–æ–¥–µ–ª–µ–π:** {len(models)}

**–£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏:**
"""
        for model in models[:10]:
            text += f"‚Ä¢ `{model}`\n"

        if len(models) > 10:
            text += f"... –∏ –µ—â—ë {len(models) - 10}"
    else:
        text = f"""‚ùå **Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞**

**–û—à–∏–±–∫–∞:** {status.get('error', 'Unknown')}

**–ö–∞–∫ –∑–∞–ø—É—Å—Ç–∏—Ç—å:**
1. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ Ollama: `curl -fsSL https://ollama.com/install.sh | sh`
2. –°–∫–∞—á–∞–π—Ç–µ –º–æ–¥–µ–ª—å: `ollama pull llama3.1:8b`
3. –ó–∞–ø—É—Å—Ç–∏—Ç–µ: `ollama serve`"""

    await query.edit_message_text(text, parse_mode="Markdown")


async def ai_clear_callback(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Callback –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –∏—Å—Ç–æ—Ä–∏–∏."""
    query = update.callback_query
    if not query:
        return

    await query.answer("–ò—Å—Ç–æ—Ä–∏—è –æ—á–∏—â–µ–Ω–∞")

    handler = get_ai_handler()
    user_id = update.effective_user.id
    handler.clear_history(user_id)

    await query.edit_message_text(
        "üóëÔ∏è –ò—Å—Ç–æ—Ä–∏—è —Ä–∞–∑–≥–æ–≤–æ—Ä–∞ –æ—á–∏—â–µ–Ω–∞.\n\n"
        "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ /ai –¥–ª—è –Ω–æ–≤–æ–≥–æ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞.",
        parse_mode="Markdown",
    )


# –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤
def register_ai_handlers(application) -> None:
    """–†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è AI –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤."""
    from telegram.ext import CallbackQueryHandler, CommandHandler

    application.add_handler(CommandHandler("ai", ai_command))
    application.add_handler(CommandHandler("ai_chat", ai_chat_command))
    application.add_handler(CommandHandler("ai_models", ai_models_command))
    application.add_handler(CommandHandler("ai_set_model", ai_set_model_command))
    application.add_handler(CommandHandler("ai_analyze", ai_analyze_command))
    application.add_handler(CommandHandler("ai_recommend", ai_recommend_command))

    application.add_handler(CallbackQueryHandler(ai_status_callback, pattern="^ai_status$"))
    application.add_handler(CallbackQueryHandler(ai_clear_callback, pattern="^ai_clear$"))

    logger.info("ai_handlers_registered")
