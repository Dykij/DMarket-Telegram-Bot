"""
Comprehensive tests for Llama 3.1 8B Integration.

–¢–µ—Å—Ç—ã –ø—Ä–æ–≤–µ—Ä—è—é—Ç:
1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
2. –î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Ollama –∏ –º–æ–¥–µ–ª–∏
3. –í—Å–µ —Ç–∏–ø—ã –∑–∞–¥–∞—á (market_analysis, price_prediction, etc.)
4. –û–±—Ä–∞–±–æ—Ç–∫—É –æ—à–∏–±–æ–∫
5. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
6. –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
"""

import asyncio
import json
from datetime import datetime
from unittest.mock import AsyncMock, MagicMock, patch

import pytest

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –º–æ–¥—É–ª—å
from src.ml.llama_integration import (
    LlamaConfig,
    LlamaIntegration,
    LlamaResponse,
    LlamaTaskType,
    TASK_PROMPTS,
    get_llama,
    init_llama,
)


class TestLlamaConfig:
    """–¢–µ—Å—Ç—ã –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ Llama."""
    
    def test_default_config(self):
        """–¢–µ—Å—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é."""
        config = LlamaConfig()
        
        assert config.model_name == "llama3.1:8b"
        assert config.ollama_url == "http://localhost:11434"
        assert config.temperature == 0.7
        assert config.top_p == 0.9
        assert config.max_tokens == 1024
        assert config.timeout == 120.0
        assert config.quantization == "Q4_K_M"
        assert config.context_length == 8192
        assert config.min_vram_gb == 6
        assert config.recommended_vram_gb == 8
    
    def test_custom_config(self):
        """–¢–µ—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏."""
        config = LlamaConfig(
            model_name="qwen2.5:7b",
            ollama_url="http://192.168.1.100:11434",
            temperature=0.5,
            max_tokens=2048,
        )
        
        assert config.model_name == "qwen2.5:7b"
        assert config.ollama_url == "http://192.168.1.100:11434"
        assert config.temperature == 0.5
        assert config.max_tokens == 2048


class TestLlamaResponse:
    """–¢–µ—Å—Ç—ã –æ—Ç–≤–µ—Ç–∞ Llama."""
    
    def test_successful_response(self):
        """–¢–µ—Å—Ç —É—Å–ø–µ—à–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞."""
        response = LlamaResponse(
            success=True,
            response="–ê–Ω–∞–ª–∏–∑ —Ä—ã–Ω–∫–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –≤–æ—Å—Ö–æ–¥—è—â–∏–π —Ç—Ä–µ–Ω–¥.",
            task_type=LlamaTaskType.MARKET_ANALYSIS,
            tokens_used=150,
            processing_time_ms=2500.0,
        )
        
        assert response.success is True
        assert "–≤–æ—Å—Ö–æ–¥—è—â–∏–π —Ç—Ä–µ–Ω–¥" in response.response
        assert response.task_type == LlamaTaskType.MARKET_ANALYSIS
        assert response.tokens_used == 150
        assert response.processing_time_ms == 2500.0
        assert response.error is None
    
    def test_failed_response(self):
        """–¢–µ—Å—Ç –Ω–µ—É–¥–∞—á–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞."""
        response = LlamaResponse(
            success=False,
            response="",
            task_type=LlamaTaskType.GENERAL_CHAT,
            error="Connection refused",
        )
        
        assert response.success is False
        assert response.response == ""
        assert response.error == "Connection refused"


class TestLlamaTaskType:
    """–¢–µ—Å—Ç—ã —Ç–∏–ø–æ–≤ –∑–∞–¥–∞—á."""
    
    def test_all_task_types_have_prompts(self):
        """–ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –¥–ª—è –≤—Å–µ—Ö —Ç–∏–ø–æ–≤ –∑–∞–¥–∞—á –µ—Å—Ç—å –ø—Ä–æ–º–ø—Ç—ã."""
        for task_type in LlamaTaskType:
            assert task_type in TASK_PROMPTS, f"Missing prompt for {task_type}"
    
    def test_task_type_values(self):
        """–¢–µ—Å—Ç –∑–Ω–∞—á–µ–Ω–∏–π —Ç–∏–ø–æ–≤ –∑–∞–¥–∞—á."""
        assert LlamaTaskType.MARKET_ANALYSIS == "market_analysis"
        assert LlamaTaskType.PRICE_PREDICTION == "price_prediction"
        assert LlamaTaskType.ARBITRAGE_RECOMMENDATION == "arbitrage_recommendation"
        assert LlamaTaskType.TRADING_ADVICE == "trading_advice"
        assert LlamaTaskType.GENERAL_CHAT == "general_chat"
        assert LlamaTaskType.ITEM_EVALUATION == "item_evaluation"
        assert LlamaTaskType.RISK_ASSESSMENT == "risk_assessment"


class TestLlamaIntegrationInit:
    """–¢–µ—Å—Ç—ã –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ LlamaIntegration."""
    
    def test_default_initialization(self):
        """–¢–µ—Å—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é."""
        llama = LlamaIntegration()
        
        assert llama.config.model_name == "llama3.1:8b"
        assert llama._client is None
        assert llama._is_available is None
        assert llama.stats["total_requests"] == 0
    
    def test_custom_config_initialization(self):
        """–¢–µ—Å—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π."""
        config = LlamaConfig(model_name="mistral:7b")
        llama = LlamaIntegration(config)
        
        assert llama.config.model_name == "mistral:7b"


class TestLlamaIntegrationAvailability:
    """–¢–µ—Å—Ç—ã –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏."""
    
    @pytest.mark.asyncio
    async def test_check_availability_success(self):
        """–¢–µ—Å—Ç —É—Å–ø–µ—à–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏."""
        llama = LlamaIntegration()
        
        mock_response = MagicMock()
        mock_response.status_code = 200
        mock_response.json.return_value = {
            "models": [
                {"name": "llama3.1:8b"},
                {"name": "mistral:7b"},
            ]
        }
        
        with patch("httpx.AsyncClient") as mock_client_class:
            mock_client = AsyncMock()
            mock_client.get = AsyncMock(return_value=mock_response)
            mock_client.is_closed = False
            mock_client_class.return_value.__aenter__ = AsyncMock(return_value=mock_client)
            mock_client_class.return_value = mock_client
            
            available = await llama.check_availability(force=True)
            
            assert available is True
            assert llama._is_available is True
        
        await llama.close()
    
    @pytest.mark.asyncio
    async def test_check_availability_model_not_found(self):
        """–¢–µ—Å—Ç –∫–æ–≥–¥–∞ –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞."""
        llama = LlamaIntegration()
        
        mock_response = MagicMock()
        mock_response.status_code = 200
        mock_response.json.return_value = {
            "models": [{"name": "qwen2.5:7b"}]  # llama3.1:8b –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç
        }
        
        with patch("httpx.AsyncClient") as mock_client_class:
            mock_client = AsyncMock()
            mock_client.get = AsyncMock(return_value=mock_response)
            mock_client.is_closed = False
            mock_client_class.return_value = mock_client
            
            available = await llama.check_availability(force=True)
            
            assert available is False
        
        await llama.close()
    
    @pytest.mark.asyncio
    async def test_check_availability_connection_error(self):
        """–¢–µ—Å—Ç –æ—à–∏–±–∫–∏ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è."""
        llama = LlamaIntegration()
        
        with patch("httpx.AsyncClient") as mock_client_class:
            mock_client = AsyncMock()
            mock_client.get = AsyncMock(side_effect=Exception("Connection refused"))
            mock_client.is_closed = False
            mock_client_class.return_value = mock_client
            
            available = await llama.check_availability(force=True)
            
            assert available is False
            assert llama._is_available is False
        
        await llama.close()


class TestLlamaIntegrationExecuteTask:
    """–¢–µ—Å—Ç—ã –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á."""
    
    @pytest.mark.asyncio
    async def test_execute_market_analysis_task(self):
        """–¢–µ—Å—Ç –∑–∞–¥–∞—á–∏ –∞–Ω–∞–ª–∏–∑–∞ —Ä—ã–Ω–∫–∞."""
        llama = LlamaIntegration()
        
        mock_check = AsyncMock(return_value=True)
        llama.check_availability = mock_check
        
        mock_response = MagicMock()
        mock_response.status_code = 200
        mock_response.json.return_value = {
            "message": {
                "content": "üìä –¢–†–ï–ù–î: –≤–æ—Å—Ö–æ–¥—è—â–∏–π\nüìà –°–ò–õ–ê –¢–†–ï–ù–î–ê: —Å–∏–ª—å–Ω—ã–π\nüí∞ –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–Ø: –ø–æ–∫—É–ø–∞—Ç—å"
            },
            "eval_count": 100,
            "prompt_eval_count": 50,
        }
        
        with patch("httpx.AsyncClient") as mock_client_class:
            mock_client = AsyncMock()
            mock_client.post = AsyncMock(return_value=mock_response)
            mock_client.is_closed = False
            mock_client_class.return_value = mock_client
            llama._client = mock_client
            
            result = await llama.execute_task(
                LlamaTaskType.MARKET_ANALYSIS,
                "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ä—ã–Ω–æ–∫ CS:GO",
            )
            
            assert result.success is True
            assert "–≤–æ—Å—Ö–æ–¥—è—â–∏–π" in result.response
            assert result.task_type == LlamaTaskType.MARKET_ANALYSIS
            assert result.tokens_used == 150
        
        await llama.close()
    
    @pytest.mark.asyncio
    async def test_execute_task_with_context(self):
        """–¢–µ—Å—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á–∏ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º."""
        llama = LlamaIntegration()
        
        mock_check = AsyncMock(return_value=True)
        llama.check_availability = mock_check
        
        mock_response = MagicMock()
        mock_response.status_code = 200
        mock_response.json.return_value = {
            "message": {"content": "–ê–Ω–∞–ª–∏–∑ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö..."},
            "eval_count": 80,
            "prompt_eval_count": 120,
        }
        
        with patch("httpx.AsyncClient") as mock_client_class:
            mock_client = AsyncMock()
            mock_client.post = AsyncMock(return_value=mock_response)
            mock_client.is_closed = False
            mock_client_class.return_value = mock_client
            llama._client = mock_client
            
            context = {
                "item": "AK-47 | Redline",
                "prices": [15.50, 15.80, 16.20],
                "trend": "up",
            }
            
            result = await llama.execute_task(
                LlamaTaskType.PRICE_PREDICTION,
                "–î–∞–π –ø—Ä–æ–≥–Ω–æ–∑ —Ü–µ–Ω—ã",
                context=context,
            )
            
            assert result.success is True
            assert result.metadata["context_provided"] is True
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç –±—ã–ª –ø–µ—Ä–µ–¥–∞–Ω –≤ –∑–∞–ø—Ä–æ—Å
            call_args = mock_client.post.call_args
            request_body = call_args.kwargs["json"]
            user_message = request_body["messages"][-1]["content"]
            assert "AK-47" in user_message
        
        await llama.close()
    
    @pytest.mark.asyncio
    async def test_execute_task_ollama_unavailable(self):
        """–¢–µ—Å—Ç –∫–æ–≥–¥–∞ Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞."""
        llama = LlamaIntegration()
        
        mock_check = AsyncMock(return_value=False)
        llama.check_availability = mock_check
        
        result = await llama.execute_task(
            LlamaTaskType.GENERAL_CHAT,
            "–ü—Ä–∏–≤–µ—Ç!",
        )
        
        assert result.success is False
        assert "–Ω–µ–¥–æ—Å—Ç—É–ø–Ω" in result.error.lower() or "–∑–∞–ø—É—Å—Ç–∏—Ç–µ" in result.error.lower()
        await llama.close()
    
    @pytest.mark.asyncio
    async def test_execute_task_timeout(self):
        """–¢–µ—Å—Ç —Ç–∞–π–º–∞—É—Ç–∞ –∑–∞–ø—Ä–æ—Å–∞."""
        llama = LlamaIntegration()
        
        mock_check = AsyncMock(return_value=True)
        llama.check_availability = mock_check
        
        import httpx as real_httpx
        
        with patch("httpx.AsyncClient") as mock_client_class:
            mock_client = AsyncMock()
            mock_client.post = AsyncMock(side_effect=real_httpx.TimeoutException("Timeout"))
            mock_client.is_closed = False
            mock_client_class.return_value = mock_client
            llama._client = mock_client
            
            result = await llama.execute_task(
                LlamaTaskType.GENERAL_CHAT,
                "–¢–µ—Å—Ç —Ç–∞–π–º–∞—É—Ç–∞",
            )
            
            assert result.success is False
            assert "—Ç–∞–π–º–∞—É—Ç" in result.error.lower()
        
        await llama.close()


class TestLlamaIntegrationHighLevelMethods:
    """–¢–µ—Å—Ç—ã –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã—Ö –º–µ—Ç–æ–¥–æ–≤."""
    
    @pytest.mark.asyncio
    async def test_analyze_market(self):
        """–¢–µ—Å—Ç –º–µ—Ç–æ–¥–∞ analyze_market."""
        llama = LlamaIntegration()
        
        mock_execute = AsyncMock(return_value=LlamaResponse(
            success=True,
            response="–†—ã–Ω–æ–∫ CS:GO –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ä–æ—Å—Ç",
            task_type=LlamaTaskType.MARKET_ANALYSIS,
        ))
        llama.execute_task = mock_execute
        
        result = await llama.analyze_market(
            "csgo",
            market_data={"volume": 10000, "trend": "up"},
        )
        
        assert result.success is True
        mock_execute.assert_called_once()
        call_args = mock_execute.call_args
        assert call_args.args[0] == LlamaTaskType.MARKET_ANALYSIS
        assert "csgo" in call_args.args[1].lower()
        await llama.close()
    
    @pytest.mark.asyncio
    async def test_predict_price(self):
        """–¢–µ—Å—Ç –º–µ—Ç–æ–¥–∞ predict_price."""
        llama = LlamaIntegration()
        
        mock_execute = AsyncMock(return_value=LlamaResponse(
            success=True,
            response="üéØ –ü–†–û–ì–ù–û–ó 24—á: $16.50 (+5%)",
            task_type=LlamaTaskType.PRICE_PREDICTION,
        ))
        llama.execute_task = mock_execute
        
        price_history = [
            {"date": "2026-01-01", "price": 15.0},
            {"date": "2026-01-02", "price": 15.5},
            {"date": "2026-01-03", "price": 15.7},
        ]
        
        result = await llama.predict_price("AK-47 | Redline", price_history)
        
        assert result.success is True
        call_args = mock_execute.call_args
        assert call_args.args[0] == LlamaTaskType.PRICE_PREDICTION
        await llama.close()
    
    @pytest.mark.asyncio
    async def test_find_arbitrage(self):
        """–¢–µ—Å—Ç –º–µ—Ç–æ–¥–∞ find_arbitrage."""
        llama = LlamaIntegration()
        
        mock_execute = AsyncMock(return_value=LlamaResponse(
            success=True,
            response="üíé –í–û–ó–ú–û–ñ–ù–û–°–¢–¨: –ö—É–ø–∏—Ç—å –Ω–∞ DMarket, –ø—Ä–æ–¥–∞—Ç—å –Ω–∞ Waxpeer\nüí∞ –ß–ò–°–¢–ê–Ø –ü–†–ò–ë–´–õ–¨: $2.50",
            task_type=LlamaTaskType.ARBITRAGE_RECOMMENDATION,
        ))
        llama.execute_task = mock_execute
        
        opportunities = [
            {
                "item": "AWP | Asiimov",
                "buy_price": 45.0,
                "sell_price": 52.0,
                "platform_buy": "dmarket",
                "platform_sell": "waxpeer",
            },
        ]
        
        result = await llama.find_arbitrage(opportunities)
        
        assert result.success is True
        call_args = mock_execute.call_args
        assert call_args.args[0] == LlamaTaskType.ARBITRAGE_RECOMMENDATION
        await llama.close()
    
    @pytest.mark.asyncio
    async def test_get_trading_advice(self):
        """–¢–µ—Å—Ç –º–µ—Ç–æ–¥–∞ get_trading_advice."""
        llama = LlamaIntegration()
        
        mock_execute = AsyncMock(return_value=LlamaResponse(
            success=True,
            response="üéØ –°–û–í–ï–¢: –î–∏–≤–µ—Ä—Å–∏—Ñ–∏—Ü–∏—Ä—É–π—Ç–µ –ø–æ—Ä—Ç—Ñ–µ–ª—å",
            task_type=LlamaTaskType.TRADING_ADVICE,
        ))
        llama.execute_task = mock_execute
        
        portfolio = {"items": [{"name": "AWP", "value": 50}]}
        
        result = await llama.get_trading_advice(
            portfolio=portfolio,
            balance=100.0,
            risk_tolerance="medium",
        )
        
        assert result.success is True
        call_args = mock_execute.call_args
        assert call_args.args[0] == LlamaTaskType.TRADING_ADVICE
        assert "medium" in str(call_args.kwargs.get("context", {}))
        await llama.close()
    
    @pytest.mark.asyncio
    async def test_evaluate_item(self):
        """–¢–µ—Å—Ç –º–µ—Ç–æ–¥–∞ evaluate_item."""
        llama = LlamaIntegration()
        
        mock_execute = AsyncMock(return_value=LlamaResponse(
            success=True,
            response="üè∑Ô∏è –ü–†–ï–î–ú–ï–¢: AWP | Asiimov\nüí∞ –°–ü–†–ê–í–ï–î–õ–ò–í–ê–Ø –¶–ï–ù–ê: $48.00",
            task_type=LlamaTaskType.ITEM_EVALUATION,
        ))
        llama.execute_task = mock_execute
        
        result = await llama.evaluate_item(
            item_name="AWP | Asiimov",
            current_price=45.0,
            item_data={"float": 0.25, "rarity": "covert"},
        )
        
        assert result.success is True
        call_args = mock_execute.call_args
        assert call_args.args[0] == LlamaTaskType.ITEM_EVALUATION
        await llama.close()
    
    @pytest.mark.asyncio
    async def test_assess_risk(self):
        """–¢–µ—Å—Ç –º–µ—Ç–æ–¥–∞ assess_risk."""
        llama = LlamaIntegration()
        
        mock_execute = AsyncMock(return_value=LlamaResponse(
            success=True,
            response="‚ö†Ô∏è –û–ë–©–ò–ô –£–†–û–í–ï–ù–¨ –†–ò–°–ö–ê: 4/10\nüìä –í–û–õ–ê–¢–ò–õ–¨–ù–û–°–¢–¨: —Å—Ä–µ–¥–Ω—è—è",
            task_type=LlamaTaskType.RISK_ASSESSMENT,
        ))
        llama.execute_task = mock_execute
        
        portfolio = {
            "total_value": 500.0,
            "items": [
                {"name": "AWP | Asiimov", "value": 45.0},
                {"name": "AK-47 | Redline", "value": 15.0},
            ],
        }
        
        result = await llama.assess_risk(portfolio)
        
        assert result.success is True
        call_args = mock_execute.call_args
        assert call_args.args[0] == LlamaTaskType.RISK_ASSESSMENT
        await llama.close()
    
    @pytest.mark.asyncio
    async def test_chat(self):
        """–¢–µ—Å—Ç –º–µ—Ç–æ–¥–∞ chat."""
        llama = LlamaIntegration()
        
        mock_execute = AsyncMock(return_value=LlamaResponse(
            success=True,
            response="–ü—Ä–∏–≤–µ—Ç! –ß–µ–º –º–æ–≥—É –ø–æ–º–æ—á—å?",
            task_type=LlamaTaskType.GENERAL_CHAT,
        ))
        llama.execute_task = mock_execute
        
        history = [
            {"role": "user", "content": "–ü—Ä–∏–≤–µ—Ç"},
            {"role": "assistant", "content": "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ!"},
        ]
        
        result = await llama.chat("–ö–∞–∫ –¥–µ–ª–∞?", conversation_history=history)
        
        assert result.success is True
        call_args = mock_execute.call_args
        assert call_args.args[0] == LlamaTaskType.GENERAL_CHAT
        assert call_args.kwargs.get("conversation_history") == history
        await llama.close()


class TestLlamaIntegrationStatistics:
    """–¢–µ—Å—Ç—ã —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è."""
    
    @pytest.mark.asyncio
    async def test_statistics_tracking(self):
        """–¢–µ—Å—Ç –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏."""
        llama = LlamaIntegration()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞—á–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        stats = llama.get_statistics()
        assert stats["total_requests"] == 0
        assert stats["successful_requests"] == 0
        assert stats["failed_requests"] == 0
        
        # –°–∏–º—É–ª–∏—Ä—É–µ–º —É—Å–ø–µ—à–Ω—ã–π –∑–∞–ø—Ä–æ—Å
        mock_check = AsyncMock(return_value=True)
        llama.check_availability = mock_check
        
        mock_response = MagicMock()
        mock_response.status_code = 200
        mock_response.json.return_value = {
            "message": {"content": "–û—Ç–≤–µ—Ç"},
            "eval_count": 50,
            "prompt_eval_count": 30,
        }
        
        with patch("httpx.AsyncClient") as mock_client_class:
            mock_client = AsyncMock()
            mock_client.post = AsyncMock(return_value=mock_response)
            mock_client.is_closed = False
            mock_client_class.return_value = mock_client
            llama._client = mock_client
            
            await llama.execute_task(LlamaTaskType.GENERAL_CHAT, "–¢–µ—Å—Ç")
        
        stats = llama.get_statistics()
        assert stats["total_requests"] == 1
        assert stats["successful_requests"] == 1
        assert stats["total_tokens"] == 80
        assert stats["success_rate"] == 100.0
        
        await llama.close()
    
    @pytest.mark.asyncio
    async def test_statistics_on_failure(self):
        """–¢–µ—Å—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö."""
        llama = LlamaIntegration()
        
        # –°–∏–º—É–ª–∏—Ä—É–µ–º –Ω–µ—É–¥–∞—á–Ω—ã–π –∑–∞–ø—Ä–æ—Å
        mock_check = AsyncMock(return_value=False)
        llama.check_availability = mock_check
        
        await llama.execute_task(LlamaTaskType.GENERAL_CHAT, "–¢–µ—Å—Ç")
        
        stats = llama.get_statistics()
        assert stats["total_requests"] == 1
        assert stats["failed_requests"] == 1
        assert stats["success_rate"] == 0.0
        
        await llama.close()


class TestLlamaIntegrationGetModels:
    """–¢–µ—Å—Ç—ã –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ –º–æ–¥–µ–ª–µ–π."""
    
    @pytest.mark.asyncio
    async def test_get_available_models_success(self):
        """–¢–µ—Å—Ç —É—Å–ø–µ—à–Ω–æ–≥–æ –ø–æ–ª—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π."""
        llama = LlamaIntegration()
        
        mock_response = MagicMock()
        mock_response.status_code = 200
        mock_response.json.return_value = {
            "models": [
                {"name": "llama3.1:8b"},
                {"name": "mistral:7b"},
                {"name": "qwen2.5:7b"},
            ]
        }
        
        with patch("httpx.AsyncClient") as mock_client_class:
            mock_client = AsyncMock()
            mock_client.get = AsyncMock(return_value=mock_response)
            mock_client.is_closed = False
            mock_client_class.return_value = mock_client
            llama._client = mock_client
            
            models = await llama.get_available_models()
            
            assert len(models) == 3
            assert "llama3.1:8b" in models
            assert "mistral:7b" in models
        
        await llama.close()
    
    @pytest.mark.asyncio
    async def test_get_available_models_error(self):
        """–¢–µ—Å—Ç –æ—à–∏–±–∫–∏ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–µ–π."""
        llama = LlamaIntegration()
        
        with patch("httpx.AsyncClient") as mock_client_class:
            mock_client = AsyncMock()
            mock_client.get = AsyncMock(side_effect=Exception("Network error"))
            mock_client.is_closed = False
            mock_client_class.return_value = mock_client
            llama._client = mock_client
            
            models = await llama.get_available_models()
            
            assert models == []
        
        await llama.close()


class TestLlamaIntegrationGlobalInstance:
    """–¢–µ—Å—Ç—ã –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞."""
    
    def test_get_llama_creates_instance(self):
        """–¢–µ—Å—Ç —Å–æ–∑–¥–∞–Ω–∏—è –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞."""
        import src.ml.llama_integration as module
        
        # –°–±—Ä–∞—Å—ã–≤–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
        module._llama = None
        
        llama = get_llama()
        
        assert llama is not None
        assert isinstance(llama, LlamaIntegration)
    
    def test_get_llama_returns_same_instance(self):
        """–¢–µ—Å—Ç —á—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è —Ç–æ—Ç –∂–µ —ç–∫–∑–µ–º–ø–ª—è—Ä."""
        llama1 = get_llama()
        llama2 = get_llama()
        
        assert llama1 is llama2
    
    @pytest.mark.asyncio
    async def test_init_llama(self):
        """–¢–µ—Å—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π."""
        import src.ml.llama_integration as module
        
        module._llama = None
        
        config = LlamaConfig(model_name="test-model:1b")
        
        mock_check = AsyncMock(return_value=False)
        
        with patch.object(LlamaIntegration, "check_availability", mock_check):
            llama = await init_llama(config)
            
            assert llama.config.model_name == "test-model:1b"


class TestLlamaIntegrationPrompts:
    """–¢–µ—Å—Ç—ã —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤."""
    
    def test_market_analysis_prompt_contains_key_elements(self):
        """–¢–µ—Å—Ç —á—Ç–æ –ø—Ä–æ–º–ø—Ç –∞–Ω–∞–ª–∏–∑–∞ —Ä—ã–Ω–∫–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–ª—é—á–µ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã."""
        prompt = TASK_PROMPTS[LlamaTaskType.MARKET_ANALYSIS]
        
        assert "–¢–†–ï–ù–î" in prompt
        assert "–†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–Ø" in prompt
        assert "–†–ò–°–ö" in prompt
    
    def test_price_prediction_prompt_contains_timeframes(self):
        """–¢–µ—Å—Ç —á—Ç–æ –ø—Ä–æ–º–ø—Ç –ø—Ä–æ–≥–Ω–æ–∑–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä–∞–º–∫–∏."""
        prompt = TASK_PROMPTS[LlamaTaskType.PRICE_PREDICTION]
        
        assert "24—á" in prompt
        assert "7–¥" in prompt
        assert "30–¥" in prompt
    
    def test_arbitrage_prompt_contains_commissions(self):
        """–¢–µ—Å—Ç —á—Ç–æ –ø—Ä–æ–º–ø—Ç –∞—Ä–±–∏—Ç—Ä–∞–∂–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–æ–º–∏—Å—Å–∏–∏."""
        prompt = TASK_PROMPTS[LlamaTaskType.ARBITRAGE_RECOMMENDATION]
        
        assert "7%" in prompt  # DMarket
        assert "6%" in prompt  # Waxpeer
        assert "15%" in prompt  # Steam
    
    def test_general_chat_prompt_is_russian(self):
        """–¢–µ—Å—Ç —á—Ç–æ –æ–±—â–∏–π –ø—Ä–æ–º–ø—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º."""
        prompt = TASK_PROMPTS[LlamaTaskType.GENERAL_CHAT]
        
        assert "—Ä—É—Å—Å–∫" in prompt.lower()


class TestLlamaIntegrationEdgeCases:
    """–¢–µ—Å—Ç—ã –≥—Ä–∞–Ω–∏—á–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤."""
    
    @pytest.mark.asyncio
    async def test_empty_response(self):
        """–¢–µ—Å—Ç –ø—É—Å—Ç–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –æ—Ç –º–æ–¥–µ–ª–∏."""
        llama = LlamaIntegration()
        
        mock_check = AsyncMock(return_value=True)
        llama.check_availability = mock_check
        
        mock_response = MagicMock()
        mock_response.status_code = 200
        mock_response.json.return_value = {
            "message": {"content": ""},
            "eval_count": 0,
        }
        
        with patch("httpx.AsyncClient") as mock_client_class:
            mock_client = AsyncMock()
            mock_client.post = AsyncMock(return_value=mock_response)
            mock_client.is_closed = False
            mock_client_class.return_value = mock_client
            llama._client = mock_client
            
            result = await llama.execute_task(
                LlamaTaskType.GENERAL_CHAT,
                "–¢–µ—Å—Ç –ø—É—Å—Ç–æ–≥–æ –æ—Ç–≤–µ—Ç–∞",
            )
            
            assert result.success is True
            assert result.response == ""
        
        await llama.close()
    
    @pytest.mark.asyncio
    async def test_long_conversation_history_trimming(self):
        """–¢–µ—Å—Ç –æ–±—Ä–µ–∑–∫–∏ –¥–ª–∏–Ω–Ω–æ–π –∏—Å—Ç–æ—Ä–∏–∏ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞."""
        llama = LlamaIntegration()
        
        mock_check = AsyncMock(return_value=True)
        llama.check_availability = mock_check
        
        mock_response = MagicMock()
        mock_response.status_code = 200
        mock_response.json.return_value = {
            "message": {"content": "–û—Ç–≤–µ—Ç"},
            "eval_count": 10,
        }
        
        # –°–æ–∑–¥–∞–µ–º –¥–ª–∏–Ω–Ω—É—é –∏—Å—Ç–æ—Ä–∏—é (–±–æ–ª—å—à–µ 10 —Å–æ–æ–±—â–µ–Ω–∏–π)
        long_history = [
            {"role": "user" if i % 2 == 0 else "assistant", "content": f"Message {i}"}
            for i in range(25)
        ]
        
        with patch("httpx.AsyncClient") as mock_client_class:
            mock_client = AsyncMock()
            mock_client.post = AsyncMock(return_value=mock_response)
            mock_client.is_closed = False
            mock_client_class.return_value = mock_client
            llama._client = mock_client
            
            await llama.execute_task(
                LlamaTaskType.GENERAL_CHAT,
                "–¢–µ—Å—Ç –¥–ª–∏–Ω–Ω–æ–π –∏—Å—Ç–æ—Ä–∏–∏",
                conversation_history=long_history,
            )
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∏—Å—Ç–æ—Ä–∏—è –±—ã–ª–∞ –æ–±—Ä–µ–∑–∞–Ω–∞
            call_args = mock_client.post.call_args
            request_body = call_args.kwargs["json"]
            messages = request_body["messages"]
            
            # 1 system + 10 history + 1 user = 12 max
            history_messages = [m for m in messages if m["role"] != "system"]
            assert len(history_messages) <= 11  # 10 history + 1 current
        
        await llama.close()
    
    @pytest.mark.asyncio
    async def test_http_error_response(self):
        """–¢–µ—Å—Ç HTTP –æ—à–∏–±–∫–∏."""
        llama = LlamaIntegration()
        
        mock_check = AsyncMock(return_value=True)
        llama.check_availability = mock_check
        
        mock_response = MagicMock()
        mock_response.status_code = 500
        mock_response.text = "Internal Server Error"
        
        with patch("httpx.AsyncClient") as mock_client_class:
            mock_client = AsyncMock()
            mock_client.post = AsyncMock(return_value=mock_response)
            mock_client.is_closed = False
            mock_client_class.return_value = mock_client
            llama._client = mock_client
            
            result = await llama.execute_task(
                LlamaTaskType.GENERAL_CHAT,
                "–¢–µ—Å—Ç HTTP –æ—à–∏–±–∫–∏",
            )
            
            assert result.success is False
            assert "500" in result.error
        
        await llama.close()


class TestLlamaIntegrationClose:
    """–¢–µ—Å—Ç—ã –∑–∞–∫—Ä—ã—Ç–∏—è —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è."""
    
    @pytest.mark.asyncio
    async def test_close_with_client(self):
        """–¢–µ—Å—Ç –∑–∞–∫—Ä—ã—Ç–∏—è —Å –∞–∫—Ç–∏–≤–Ω—ã–º –∫–ª–∏–µ–Ω—Ç–æ–º."""
        llama = LlamaIntegration()
        
        mock_client = AsyncMock()
        mock_client.is_closed = False
        mock_client.aclose = AsyncMock()
        llama._client = mock_client
        
        await llama.close()
        
        mock_client.aclose.assert_called_once()
        assert llama._client is None
    
    @pytest.mark.asyncio
    async def test_close_without_client(self):
        """–¢–µ—Å—Ç –∑–∞–∫—Ä—ã—Ç–∏—è –±–µ–∑ –∫–ª–∏–µ–Ω—Ç–∞."""
        llama = LlamaIntegration()
        
        # –ù–µ –¥–æ–ª–∂–Ω–æ –≤—ã–∑—ã–≤–∞—Ç—å –æ—à–∏–±–∫—É
        await llama.close()
        
        assert llama._client is None


# –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã (–∑–∞–ø—É—Å–∫–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ Ollama)
class TestLlamaIntegrationRealOllama:
    """
    –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã —Å —Ä–µ–∞–ª—å–Ω—ã–º Ollama.
    
    –≠—Ç–∏ —Ç–µ—Å—Ç—ã –ø—Ä–æ–ø—É—Å–∫–∞—é—Ç—Å—è –µ—Å–ª–∏ Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞.
    """
    
    @pytest.fixture
    async def real_llama(self):
        """–§–∏–∫—Å—Ç—É—Ä–∞ –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è."""
        llama = LlamaIntegration()
        available = await llama.check_availability(force=True)
        if not available:
            pytest.skip("Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤")
        yield llama
        await llama.close()
    
    @pytest.mark.asyncio
    @pytest.mark.integration
    async def test_real_chat(self, real_llama):
        """–¢–µ—Å—Ç —Ä–µ–∞–ª—å–Ω–æ–≥–æ —á–∞—Ç–∞ (—Ç—Ä–µ–±—É–µ—Ç Ollama)."""
        result = await real_llama.chat("–ü—Ä–∏–≤–µ—Ç! –¢—ã —Ä–∞–±–æ—Ç–∞–µ—à—å?")
        
        assert result.success is True
        assert len(result.response) > 0
        assert result.tokens_used > 0
        assert result.processing_time_ms > 0
    
    @pytest.mark.asyncio
    @pytest.mark.integration
    async def test_real_market_analysis(self, real_llama):
        """–¢–µ—Å—Ç —Ä–µ–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Ä—ã–Ω–∫–∞ (—Ç—Ä–µ–±—É–µ—Ç Ollama)."""
        result = await real_llama.analyze_market(
            "csgo",
            market_data={"volume": 5000, "avg_price": 15.0},
        )
        
        assert result.success is True
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –æ—Ç–≤–µ—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç —ç–ª–µ–º–µ–Ω—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
        response_lower = result.response.lower()
        assert any(word in response_lower for word in ["—Ç—Ä–µ–Ω–¥", "—Ä—ã–Ω–æ–∫", "—Ü–µ–Ω", "–∞–Ω–∞–ª–∏–∑"])
