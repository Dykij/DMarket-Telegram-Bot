"""EXAMPLE: Advanced migration patterns.

This file demonstrates various advanced migration patterns and best practices.
Rename to .py to use as a template.

Revision ID: example_advanced
Revises: 001
Create Date: 2025-11-19 12:00:00

"""

import sqlalchemy as sa
from alembic import op
from sqlalchemy import table, column

# revision identifiers, used by Alembic.
revision: str = "example_advanced"
down_revision: str | None = "001"
branch_labels: str | tuple[str, ...] | None = None
depends_on: str | tuple[str, ...] | None = None


# ============================================================================
# PATTERN 1: Adding a column with default value (PostgreSQL optimized)
# ============================================================================


def upgrade_pattern_1_add_column_with_default() -> None:
    """Add column with default value without locking table.

    For PostgreSQL 11+, this is done in two steps to avoid table rewrite:
    1. Add column as nullable
    2. Set default for future inserts
    3. Backfill existing rows (in batches)
    4. Make column NOT NULL
    """
    # Step 1: Add column as nullable
    op.add_column(
        "users",
        sa.Column("email_verified", sa.Boolean(), nullable=True),
    )

    # Step 2: Set default for future inserts (doesn't rewrite table)
    op.alter_column(
        "users",
        "email_verified",
        server_default=sa.text("false"),
    )

    # Step 3: Backfill existing rows (in batches to avoid long locks)
    conn = op.get_bind()
    users = table("users", column("id", sa.String), column("email_verified", sa.Boolean))

    batch_size = 1000
    offset = 0

    while True:
        # Update in batches
        stmt = sa.update(users).where(
            users.c.id.in_(
                sa.select(users.c.id)
                .where(users.c.email_verified.is_(None))
                .limit(batch_size)
                .offset(offset)
            )
        ).values(email_verified=False)

        result = conn.execute(stmt)
        if result.rowcount == 0:
            break
        offset += batch_size

    # Step 4: Make column NOT NULL (now safe since all rows have values)
    op.alter_column("users", "email_verified", nullable=False)


# ============================================================================
# PATTERN 2: Creating an ENUM type (PostgreSQL)
# ============================================================================


def upgrade_pattern_2_create_enum() -> None:
    """Create and use PostgreSQL ENUM type.

    Important: Always create enum before using it in columns.
    """
    # Create ENUM type
    user_role_enum = sa.Enum(
        "user",
        "moderator",
        "admin",
        name="user_role",
        create_type=True,
    )

    # Add column using the enum
    op.add_column(
        "users",
        sa.Column(
            "role",
            user_role_enum,
            nullable=False,
            server_default="user",
        ),
    )


def downgrade_pattern_2_drop_enum() -> None:
    """Drop column and ENUM type."""
    # Drop column first
    op.drop_column("users", "role")

    # Then drop enum type
    op.execute("DROP TYPE IF EXISTS user_role")


# ============================================================================
# PATTERN 3: Adding a foreign key with data validation
# ============================================================================


def upgrade_pattern_3_add_foreign_key() -> None:
    """Add foreign key with data validation.

    Steps:
    1. Add column
    2. Populate with valid data
    3. Add foreign key constraint
    """
    # Step 1: Add column
    op.add_column(
        "trade_history",
        sa.Column("user_id", sa.String(36), nullable=True),
    )

    # Step 2: Populate from existing data
    conn = op.get_bind()
    trade_history = table(
        "trade_history",
        column("id", sa.Integer),
        column("telegram_id", sa.BigInteger),
        column("user_id", sa.String),
    )
    users = table(
        "users",
        column("id", sa.String),
        column("telegram_id", sa.BigInteger),
    )

    # Update trade_history.user_id from users table
    stmt = (
        sa.update(trade_history)
        .values(
            user_id=sa.select(users.c.id)
            .where(users.c.telegram_id == trade_history.c.telegram_id)
            .scalar_subquery()
        )
        .where(trade_history.c.user_id.is_(None))
    )
    conn.execute(stmt)

    # Step 3: Make NOT NULL
    op.alter_column("trade_history", "user_id", nullable=False)

    # Step 4: Add foreign key
    op.create_foreign_key(
        "fk_trade_history_user_id_users",
        "trade_history",
        "users",
        ["user_id"],
        ["id"],
        ondelete="CASCADE",
    )

    # Step 5: Drop old telegram_id column (optional)
    # op.drop_column('trade_history', 'telegram_id')


# ============================================================================
# PATTERN 4: Creating an index concurrently (PostgreSQL)
# ============================================================================


def upgrade_pattern_4_concurrent_index() -> None:
    """Create index without locking the table.

    For PostgreSQL, use CONCURRENTLY to avoid table locks.
    NOTE: This cannot be run inside a transaction.
    """
    # Set lock timeout to prevent long-running locks
    op.execute("SET lock_timeout = '5s'")

    # Create index concurrently (PostgreSQL only)
    op.create_index(
        "ix_users_email_verified_created_at",
        "users",
        ["email_verified", "created_at"],
        postgresql_concurrently=True,
    )


def downgrade_pattern_4_drop_index() -> None:
    """Drop index concurrently."""
    op.drop_index(
        "ix_users_email_verified_created_at",
        table_name="users",
        postgresql_concurrently=True,
    )


# ============================================================================
# PATTERN 5: Renaming a column (SQLite-compatible)
# ============================================================================


def upgrade_pattern_5_rename_column() -> None:
    """Rename column (SQLite-compatible with batch mode)."""
    # For SQLite, use batch mode
    with op.batch_alter_table("users", schema=None) as batch_op:
        batch_op.alter_column(
            "old_column_name",
            new_column_name="new_column_name",
        )


# ============================================================================
# PATTERN 6: Adding JSONB column with GIN index (PostgreSQL)
# ============================================================================


def upgrade_pattern_6_jsonb_with_index() -> None:
    """Add JSONB column with GIN index for fast lookups."""
    # Add JSONB column
    op.add_column(
        "users",
        sa.Column("metadata", sa.dialects.postgresql.JSONB, nullable=True),
    )

    # Create GIN index for JSONB queries
    op.execute(
        """
        CREATE INDEX ix_users_metadata_gin
        ON users USING GIN (metadata)
        """
    )


# ============================================================================
# PATTERN 7: Data migration with validation
# ============================================================================


def upgrade_pattern_7_data_migration_with_validation() -> None:
    """Migrate data with validation and error handling."""
    conn = op.get_bind()

    users = table(
        "users",
        column("id", sa.String),
        column("old_balance", sa.String),
        column("new_balance", sa.Float),
    )

    # Get all users
    stmt = sa.select(users.c.id, users.c.old_balance)
    all_users = conn.execute(stmt).fetchall()

    errors = []

    for user in all_users:
        try:
            # Convert string balance to float
            balance = float(user.old_balance.replace("$", "").replace(",", ""))

            # Update new column
            update_stmt = (
                sa.update(users)
                .where(users.c.id == user.id)
                .values(new_balance=balance)
            )
            conn.execute(update_stmt)

        except (ValueError, AttributeError) as e:
            errors.append(f"User {user.id}: {e!s}")

    if errors:
        # Log errors or raise exception
        print(f"Migration completed with {len(errors)} errors:")
        for error in errors:
            print(f"  - {error}")


# ============================================================================
# PATTERN 8: Adding table comment (PostgreSQL)
# ============================================================================


def upgrade_pattern_8_add_table_comment() -> None:
    """Add comment to table and columns for documentation."""
    # Table comment
    op.execute(
        """
        COMMENT ON TABLE users IS
        'Stores user information including Telegram credentials and preferences'
        """
    )

    # Column comments
    op.execute(
        """
        COMMENT ON COLUMN users.telegram_id IS
        'Unique Telegram user identifier'
        """
    )
    op.execute(
        """
        COMMENT ON COLUMN users.dmarket_api_key_encrypted IS
        'Encrypted DMarket API public key'
        """
    )


# ============================================================================
# PATTERN 9: Partitioning a large table (PostgreSQL 10+)
# ============================================================================


def upgrade_pattern_9_partition_table() -> None:
    """Partition a large table by date (PostgreSQL).

    This is useful for tables with time-series data.
    """
    # Create partitioned table
    op.execute(
        """
        CREATE TABLE trade_history_partitioned (
            id SERIAL,
            user_id VARCHAR(36) NOT NULL,
            trade_type VARCHAR(50) NOT NULL,
            created_at TIMESTAMP NOT NULL,
            PRIMARY KEY (id, created_at)
        ) PARTITION BY RANGE (created_at)
        """
    )

    # Create partitions for each month
    op.execute(
        """
        CREATE TABLE trade_history_2025_01
        PARTITION OF trade_history_partitioned
        FOR VALUES FROM ('2025-01-01') TO ('2025-02-01')
        """
    )
    op.execute(
        """
        CREATE TABLE trade_history_2025_02
        PARTITION OF trade_history_partitioned
        FOR VALUES FROM ('2025-02-01') TO ('2025-03-01')
        """
    )


# ============================================================================
# PATTERN 10: Squashing migrations
# ============================================================================


def upgrade_pattern_10_squash_migrations() -> None:
    """Example of squashed migration combining multiple old migrations.

    When you have many old migrations, you can squash them into one.
    This is the combined result of migrations 001-020.
    """
    # Create all tables at once
    op.create_table(
        "users",
        sa.Column("id", sa.String(36), primary_key=True),
        sa.Column("telegram_id", sa.BigInteger, nullable=False, unique=True),
        sa.Column("username", sa.String(255)),
        sa.Column("email_verified", sa.Boolean, server_default="false"),
        sa.Column("role", sa.Enum("user", "moderator", "admin", name="user_role")),
        sa.Column("created_at", sa.DateTime, nullable=False),
    )

    # Create all indexes at once
    op.create_index("ix_users_telegram_id", "users", ["telegram_id"])
    op.create_index("ix_users_email_verified", "users", ["email_verified"])


# ============================================================================
# Main upgrade/downgrade functions
# ============================================================================


def upgrade() -> None:
    """Upgrade database schema.

    Uncomment the pattern you want to use.
    """
    # Choose which pattern to apply:
    # upgrade_pattern_1_add_column_with_default()
    # upgrade_pattern_2_create_enum()
    # upgrade_pattern_3_add_foreign_key()
    # upgrade_pattern_4_concurrent_index()
    # upgrade_pattern_5_rename_column()
    # upgrade_pattern_6_jsonb_with_index()
    # upgrade_pattern_7_data_migration_with_validation()
    # upgrade_pattern_8_add_table_comment()
    # upgrade_pattern_9_partition_table()
    # upgrade_pattern_10_squash_migrations()
    pass


def downgrade() -> None:
    """Downgrade database schema.

    Always implement downgrade to allow rollback.
    """
    # Implement corresponding downgrades
    pass
