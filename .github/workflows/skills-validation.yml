name: Skills Validation

on:
  push:
    paths:
      - "**/SKILL_*.md"
      - "**/marketplace.json"
      - ".vscode/skills.json"
      - ".mcp.json"
      - "config/hooks.yaml"
      - "src/utils/skill_*.py"
  pull_request:
    paths:
      - "**/SKILL_*.md"
      - "**/marketplace.json"
      - ".vscode/skills.json"
      - ".mcp.json"
      - "config/hooks.yaml"
      - "src/utils/skill_*.py"
  workflow_dispatch:
    inputs:
      full_validation:
        description: "Run full validation including performance tests"
        required: false
        default: false
        type: boolean

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  PYTHON_VERSION: "3.12"
  SKILLS_CONFIG: ".vscode/skills.json"

jobs:
  validate-skills:
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: "pip"

      - name: Cache validation tools
        uses: actions/cache@v4
        with:
          path: ~/.cache/skills-validation
          key: skills-validation-${{ hashFiles('scripts/validate_*.py') }}
          restore-keys: |
            skills-validation-

      - name: Install dependencies
        run: |
          pip install pyyaml jsonschema click tabulate rich structlog

      - name: Validate SKILL.md files
        run: |
          python scripts/validate_skills.py

      - name: Validate marketplace.json files
        run: |
          python scripts/validate_marketplace.py || true

      - name: Check dependency graph
        run: |
          python scripts/skills_composition.py check || true

      - name: Validate MCP configuration
        run: |
          python -c "
          import json
          with open('.mcp.json') as f:
              config = json.load(f)
          print(f'MCP Config valid: {len(config.get(\"servers\", {}))} servers')
          for name, server in config.get('servers', {}).items():
              print(f'  - {name}: {server.get(\"command\", \"N/A\")}')
          "

      - name: Validate hooks configuration
        run: |
          python -c "
          import yaml
          with open('config/hooks.yaml') as f:
              hooks = yaml.safe_load(f)
          print(f'Hooks valid: {len(hooks.get(\"hooks\", []))} hooks defined')
          "

      - name: Generate skills report
        id: report
        run: |
          python scripts/generate_skills_report.py
          echo "report_generated=true" >> $GITHUB_OUTPUT
        continue-on-error: true

      - name: Upload skills report
        if: steps.report.outputs.report_generated == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: skills-validation-report
          path: skills_report.md
          retention-days: 30

      - name: Comment PR with report
        if: github.event_name == 'pull_request' && steps.report.outputs.report_generated == 'true'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const report = fs.readFileSync('skills_report.md', 'utf8');
            const truncatedReport = report.length > 60000 ? report.substring(0, 60000) + '\n\n... (truncated)' : report;
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `## ðŸŽ¯ Skills Validation Report\n\n${truncatedReport}`
            });

  performance-test:
    needs: validate-skills
    if: ${{ github.event.inputs.full_validation == 'true' }}
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: "pip"

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install pytest-benchmark

      - name: Run skill performance tests
        run: |
          python -m pytest tests/ -v -k "skill" --benchmark-only --benchmark-json=benchmark.json || true

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: skill-benchmarks
          path: benchmark.json
          retention-days: 30
